{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Написанные человеком и проверенные эксперименты",
   "id": "9a9b2c4b62bd9ce8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Вспомогательные функции",
   "id": "b64a1ce25957b5e6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Image conversions\n",
    "# -------------------------\n",
    "\n",
    "def pil_to_np_bgr(pil: Image.Image) -> np.ndarray:\n",
    "    \"\"\"PIL RGB -> np.uint8 BGR (H,W,3) for ultralytics predictor preprocess.\"\"\"\n",
    "    if pil.mode != \"RGB\":\n",
    "        pil = pil.convert(\"RGB\")\n",
    "    rgb = np.asarray(pil)  # uint8 RGB\n",
    "    bgr = rgb[..., ::-1].copy()\n",
    "    return bgr\n",
    "\n",
    "\n",
    "def pil_to_torch_rgb01(pil: Image.Image) -> torch.Tensor:\n",
    "    \"\"\"PIL -> torch float32 RGB in [0,1], shape [3,H,W] (CPU).\"\"\"\n",
    "    if pil.mode != \"RGB\":\n",
    "        pil = pil.convert(\"RGB\")\n",
    "    arr = np.asarray(pil).astype(np.float32) / 255.0\n",
    "    t = torch.from_numpy(arr).permute(2, 0, 1).contiguous()\n",
    "    return t\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Patch application\n",
    "# -------------------------\n",
    "\n",
    "def apply_patch_to_image(\n",
    "    base_pil: Image.Image,\n",
    "    patch_pil: Image.Image,\n",
    "    position_xy: Tuple[int, int] = (0, 0),\n",
    ") -> Tuple[Image.Image, Optional[torch.Tensor], int]:\n",
    "    \"\"\"Paste patch onto base image at (x,y). Returns patched PIL, bbox_xyxy, area_px.\"\"\"\n",
    "    if base_pil.mode != \"RGB\":\n",
    "        base_pil = base_pil.convert(\"RGB\")\n",
    "    if patch_pil.mode != \"RGB\":\n",
    "        patch_pil = patch_pil.convert(\"RGB\")\n",
    "    base_w, base_h = base_pil.size\n",
    "    patch_w, patch_h = patch_pil.size\n",
    "    x, y = position_xy\n",
    "    x1 = max(0, int(x))\n",
    "    y1 = max(0, int(y))\n",
    "    x2 = min(base_w, int(x) + patch_w)\n",
    "    y2 = min(base_h, int(y) + patch_h)\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return base_pil.copy(), None, 0\n",
    "    # Crop patch if it spills outside the base image.\n",
    "    px1 = x1 - int(x)\n",
    "    py1 = y1 - int(y)\n",
    "    px2 = px1 + (x2 - x1)\n",
    "    py2 = py1 + (y2 - y1)\n",
    "    patch_crop = patch_pil.crop((px1, py1, px2, py2))\n",
    "    out = base_pil.copy()\n",
    "    out.paste(patch_crop, (x1, y1))\n",
    "    bbox = torch.tensor([float(x1), float(y1), float(x2), float(y2)], dtype=torch.float32)\n",
    "    area = int((x2 - x1) * (y2 - y1))\n",
    "    return out, bbox, area\n",
    "\n",
    "\n",
    "def letterbox_pil(pil: Image.Image, imgsz: int, color: Tuple[int, int, int] = (114, 114, 114)) -> Image.Image:\n",
    "    \"\"\"Resize+pad image to square imgsz, matching Ultralytics letterbox behavior.\"\"\"\n",
    "    if pil.mode != \"RGB\":\n",
    "        pil = pil.convert(\"RGB\")\n",
    "    w, h = pil.size\n",
    "    r = min(imgsz / h, imgsz / w)\n",
    "    new_w = int(round(w * r))\n",
    "    new_h = int(round(h * r))\n",
    "    resized = pil.resize((new_w, new_h), Image.BILINEAR)\n",
    "    out = Image.new(\"RGB\", (imgsz, imgsz), color)\n",
    "    pad_w = int((imgsz - new_w) / 2)\n",
    "    pad_h = int((imgsz - new_h) / 2)\n",
    "    out.paste(resized, (pad_w, pad_h))\n",
    "    return out\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def collect_images(\n",
    "    folder: Path,\n",
    "    n: int,\n",
    "    extensions: tuple[str, ...] = (\".png\", \".jpg\", \".jpeg\"),\n",
    "    shuffle: bool = False,\n",
    "    seed: int | None = 42,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Берёт n изображений из папки.\n",
    "    Если shuffle=True — случайная выборка (с фиксируемым seed).\n",
    "    \"\"\"\n",
    "    files = [\n",
    "        p for p in folder.iterdir()\n",
    "        if p.is_file() and p.suffix.lower() in extensions\n",
    "    ]\n",
    "\n",
    "    if not files:\n",
    "        raise RuntimeError(f\"В папке {folder} нет изображений\")\n",
    "\n",
    "    if shuffle:\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        random.shuffle(files)\n",
    "\n",
    "    return [str(p) for p in files[:n]]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _load_ultralytics_yolo(model_path: str):\n",
    "    try:\n",
    "        from ultralytics import YOLO\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"Не удалось импортировать ultralytics. Установите пакет ultralytics или убедитесь, что среда активна.\"\n",
    "        ) from e\n",
    "    return YOLO(model_path)\n",
    "\n",
    "\n",
    "def _get_class_id_from_names(names: Dict[int, str], class_name: str) -> Optional[int]:\n",
    "    class_name = class_name.lower().strip()\n",
    "    for k, v in names.items():\n",
    "        if str(v).lower().strip() == class_name:\n",
    "            return int(k)\n",
    "    return None\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def yolo_predict_conf_scalar(\n",
    "    yolo_model,\n",
    "    pil_img: Image.Image,\n",
    "    imgsz: int,\n",
    "    target_class_id: Optional[int],\n",
    "    conf: float = 0.001,\n",
    ") -> Tuple[float, Any]:\n",
    "    \"\"\"Ultralytics predict -> scalar confidence (NMS-based) + raw Results.\"\"\"\n",
    "    bgr = pil_to_np_bgr(pil_img)\n",
    "    res = yolo_model.predict(source=bgr, imgsz=imgsz, conf=conf, verbose=False)[0]\n",
    "\n",
    "    if res.boxes is None or len(res.boxes) == 0:\n",
    "        return 0.0, res\n",
    "\n",
    "    confs = res.boxes.conf.detach().cpu().numpy()\n",
    "    clss = res.boxes.cls.detach().cpu().numpy().astype(int)\n",
    "\n",
    "    if target_class_id is None:\n",
    "        return float(confs.max(initial=0.0)), res\n",
    "\n",
    "    mask = (clss == int(target_class_id))\n",
    "    if not np.any(mask):\n",
    "        return 0.0, res\n",
    "\n",
    "    return float(confs[mask].max(initial=0.0)), res"
   ],
   "id": "76c39b776d013a90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_clean_and_patched_letterboxed(\n",
    "    base_pil: Image.Image,\n",
    "    patch_pil: Image.Image,\n",
    "    cfg: ExpConfig,\n",
    ") -> Tuple[Image.Image, Image.Image, Optional[torch.Tensor]]:\n",
    "    clean_lb = letterbox_pil(base_pil, imgsz=cfg.imgsz)\n",
    "\n",
    "    if cfg.apply_patch_after_letterbox:\n",
    "        patched_lb, bbox, _ = apply_patch_to_image(clean_lb, patch_pil, cfg.patch_xy)\n",
    "        return clean_lb, patched_lb, bbox\n",
    "\n",
    "    patched_orig, _bbox_orig, _ = apply_patch_to_image(base_pil, patch_pil, cfg.patch_xy)\n",
    "    patched_lb = letterbox_pil(patched_orig, imgsz=cfg.imgsz)\n",
    "    return clean_lb, patched_lb, None\n",
    "\n",
    "def pick_center_person_bbox_from_results(res, target_class_id: int, imgsz: int):\n",
    "    \"\"\"\n",
    "    Pick the detected person bbox (post-NMS) closest to image center.\n",
    "    Returns dict: {picked_bbox: [x1,y1,x2,y2], picked_conf: float, picked_idx: int}\n",
    "    or {} if no person found.\n",
    "    \"\"\"\n",
    "    if res is None or getattr(res, \"boxes\", None) is None or len(res.boxes) == 0:\n",
    "        return {}\n",
    "\n",
    "    xyxy = res.boxes.xyxy.detach().cpu().numpy()          # (N,4)\n",
    "    conf = res.boxes.conf.detach().cpu().numpy()          # (N,)\n",
    "    cls  = res.boxes.cls.detach().cpu().numpy().astype(int)  # (N,)\n",
    "\n",
    "    m = (cls == int(target_class_id))\n",
    "    if not np.any(m):\n",
    "        return {}\n",
    "\n",
    "    xyxy_p = xyxy[m]\n",
    "    conf_p = conf[m]\n",
    "\n",
    "    cx = 0.5 * (xyxy_p[:, 0] + xyxy_p[:, 2])\n",
    "    cy = 0.5 * (xyxy_p[:, 1] + xyxy_p[:, 3])\n",
    "    dx = cx - (float(imgsz) * 0.5)\n",
    "    dy = cy - (float(imgsz) * 0.5)\n",
    "    d2 = dx * dx + dy * dy\n",
    "\n",
    "    j = int(np.argmin(d2))\n",
    "\n",
    "    idxs = np.flatnonzero(m)\n",
    "    picked_idx = int(idxs[j])\n",
    "\n",
    "    return {\n",
    "        \"picked_bbox\": [float(v) for v in xyxy_p[j].tolist()],\n",
    "        \"picked_conf\": float(conf_p[j]),\n",
    "        \"picked_idx\": picked_idx,\n",
    "    }"
   ],
   "id": "d6b91a8034ca626d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Параметры экспериментов",
   "id": "37095b20c3646804"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "N_SUCCESS = 10\n",
    "N_FAIL = 10\n",
    "SEED=17\n",
    "\n",
    "# successful_dir = Path(\"../stats/attack_split/success\")\n",
    "# unsuccessful_dir = Path(\"../stats/attack_split/fail\")\n",
    "\n",
    "successful_dir = Path(\"successful_examples\")\n",
    "unsuccessful_dir = Path(\"unsuccessful_examples\")\n",
    "\n",
    "image_paths = (\n",
    "    collect_images(successful_dir, N_SUCCESS, shuffle=True, seed=SEED)\n",
    "    + collect_images(unsuccessful_dir, N_FAIL, shuffle=True, seed=SEED)\n",
    ")\n",
    "patch_path = \"data/patch.png\""
   ],
   "id": "f131fa9340722cf4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class ExpConfig:\n",
    "    # путь к весам (можно заменить на локальный)\n",
    "    model_path: str = \"yolo11s.pt\"\n",
    "\n",
    "    # размер входа\n",
    "    imgsz: int = 640\n",
    "\n",
    "    # устройство\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "\n",
    "    # True: накладываем патч уже на letterbox-изображение (самый честный режим)\n",
    "    # False: накладываем на оригинал, потом делаем letterbox\n",
    "    apply_patch_after_letterbox: bool = True\n",
    "\n",
    "    # позиция патча (x,y) в координатах того изображения, куда накладываем\n",
    "    patch_xy: Tuple[int, int] = (0, 0)\n",
    "\n",
    "    # целевой класс (None => берём max по всем классам)\n",
    "    target_class_name: Optional[str] = \"person\"\n",
    "\n",
    "    # успех атаки: падение скаляра уверенности >= threshold\n",
    "    success_thresh: float = 0.30\n",
    "\n",
    "    # число каналов, используемое в top-k метриках и в абляциях\n",
    "    topk_channels: int = 32\n",
    "\n",
    "    # ROI scalar reduction for attribution: \"max\" | \"logsumexp\" | \"topk_mean\"\n",
    "    roi_reduce: str = \"logsumexp\"\n",
    "\n",
    "    # temperature for logsumexp (larger -> closer to max)\n",
    "    roi_lse_temp: float = 20.0\n",
    "\n",
    "    # k for topk_mean\n",
    "    roi_topk: int = 10\n",
    "\n",
    "    # --- ROI attribution layer selection ---\n",
    "    # How many layers to keep for ROI-attribution (set 1 to mimic the previous \"single good layer\" behavior)\n",
    "    roi_keep_topk_layers: int = 1\n",
    "\n",
    "    # How many near-head conv layers to consider as candidates\n",
    "    roi_candidate_last_n_layers: int = 8\n",
    "\n",
    "    # How many examples from each group (success/fail) to use for scanning grad signal\n",
    "    roi_scan_n_per_group: int = 2\n",
    "\n",
    "    # Minimum median ma_abs.max required to accept a layer (0.0 keeps the best even if tiny)\n",
    "    roi_min_grad_strength: float = 0.0\n",
    "\n",
    "CFG = ExpConfig()"
   ],
   "id": "84112a950b4cf034",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Подсчет разности фич между атакованным и чистым примерами\n",
    "\n",
    "Берем хуками выходы предпоследних слоев модели и считаем:\n",
    "- `F_l(clean)` и `F_l(patched)` для набора слоёв `l`\n",
    "- `Δ_l = F_l(patched) - F_l(clean)`"
   ],
   "id": "727eb50011a80231"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def pick_default_conv_layers(model_torch: nn.Module, n_layers: int = 12) -> List[str]:\n",
    "    convs: List[str] = []\n",
    "    for n, m in model_torch.named_modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            convs.append(n)\n",
    "    if not convs:\n",
    "        raise RuntimeError(\"Не удалось найти Conv2d модули в модели.\")\n",
    "\n",
    "    idxs = np.linspace(0, len(convs) - 1, num=min(n_layers, len(convs))).round().astype(int)\n",
    "    picked = []\n",
    "    seen = set()\n",
    "    for i in idxs:\n",
    "        name = convs[int(i)]\n",
    "        if name not in seen:\n",
    "            picked.append(name)\n",
    "            seen.add(name)\n",
    "    return picked\n",
    "\n",
    "\n",
    "def get_module_by_name(model_torch: nn.Module, name: str) -> nn.Module:\n",
    "    d = dict(model_torch.named_modules())\n",
    "    if name not in d:\n",
    "        raise KeyError(f\"Layer '{name}' not found.\")\n",
    "    return d[name]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def torch_preprocess_letterboxed(pil_img_lb: Image.Image, device: str, dtype: torch.dtype) -> torch.Tensor:\n",
    "    \"\"\"Preprocess PIL -> BCHW tensor on the requested device/dtype.\"\"\"\n",
    "    t = pil_to_torch_rgb01(pil_img_lb)  # CPU float32 [3,H,W]\n",
    "    t = t.unsqueeze(0)  # [1,3,H,W]\n",
    "    return t.to(device=device, dtype=dtype, non_blocking=False)\n",
    "\n",
    "\n",
    "def capture_activations_for_layers(\n",
    "    model_torch: nn.Module,\n",
    "    x_bchw: torch.Tensor,\n",
    "    layer_names: List[str],\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    acts: Dict[str, torch.Tensor] = {}\n",
    "    hooks = []\n",
    "\n",
    "    def _mk_hook(name: str):\n",
    "        def _hook(_m, _inp, out):\n",
    "            # Some Ultralytics blocks can emit tuples/lists; take the first tensor.\n",
    "            if isinstance(out, (list, tuple)):\n",
    "                for o in out:\n",
    "                    if isinstance(o, torch.Tensor):\n",
    "                        out = o\n",
    "                        break\n",
    "\n",
    "            # We focus on spatial feature maps (B,C,H,W). Skip others.\n",
    "            if isinstance(out, torch.Tensor) and out.ndim == 4:\n",
    "                acts[name] = out.detach().cpu()\n",
    "        return _hook\n",
    "\n",
    "    for ln in layer_names:\n",
    "        m = get_module_by_name(model_torch, ln)\n",
    "        hooks.append(m.register_forward_hook(_mk_hook(ln)))\n",
    "    # MPS requires input and weights to share device AND dtype.\n",
    "    x_bchw = x_bchw.to(\n",
    "        device=next(model_torch.parameters()).device,\n",
    "        dtype=next(model_torch.parameters()).dtype,\n",
    "    )\n",
    "\n",
    "\n",
    "    _ = model_torch(x_bchw)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    return acts\n",
    "\n",
    "def _top_level_model_index(name: str) -> Optional[int]:\n",
    "    \"\"\"Parse 'model.<idx>....' -> idx. Returns None if not matching.\"\"\"\n",
    "    if not name.startswith(\"model.\"):\n",
    "        return None\n",
    "    parts = name.split(\".\")\n",
    "    if len(parts) < 2:\n",
    "        return None\n",
    "    try:\n",
    "        return int(parts[1])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def find_head_start_index(model_torch: nn.Module) -> int:\n",
    "    \"\"\"Heuristically find the first top-level model.<idx> belonging to the head.\"\"\"\n",
    "    head_class_names = {\n",
    "        \"Detect\", \"Segment\", \"Pose\", \"OBB\", \"Classify\", \"RTDETRDecoder\", \"WorldDetect\",\n",
    "    }\n",
    "\n",
    "    best_idx: Optional[int] = None\n",
    "\n",
    "    # 1) Prefer class-name detection\n",
    "    for n, m in model_torch.named_modules():\n",
    "        if m.__class__.__name__ in head_class_names:\n",
    "            idx = _top_level_model_index(n)\n",
    "            if idx is not None:\n",
    "                best_idx = idx if best_idx is None else min(best_idx, idx)\n",
    "\n",
    "    # 2) Fallback: name heuristics\n",
    "    if best_idx is None:\n",
    "        for n, _m in model_torch.named_modules():\n",
    "            nnm = n.lower()\n",
    "            if (\"dfl\" in nnm) or (\"detect\" in nnm) or (\"decoder\" in nnm):\n",
    "                idx = _top_level_model_index(n)\n",
    "                if idx is not None:\n",
    "                    best_idx = idx if best_idx is None else min(best_idx, idx)\n",
    "\n",
    "    # 3) If still unknown: treat everything as pre-head\n",
    "    if best_idx is None:\n",
    "        top_idxs = []\n",
    "        for n, _m in model_torch.named_modules():\n",
    "            idx = _top_level_model_index(n)\n",
    "            if idx is not None:\n",
    "                top_idxs.append(idx)\n",
    "        return max(top_idxs) if top_idxs else 10**9\n",
    "\n",
    "    return int(best_idx)"
   ],
   "id": "b9f48b075869e3ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Запуск и подсчет нужных метрик по слоям",
   "id": "f538450f242f7d4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load model\n",
    "\n",
    "yolo = _load_ultralytics_yolo(CFG.model_path)\n",
    "\n",
    "# Put Ultralytics wrapper AND underlying torch module on the same device.\n",
    "# On Apple Silicon, this ensures weights are moved to MPS as well.\n",
    "try:\n",
    "    yolo.to(CFG.device)\n",
    "except Exception:\n",
    "    # Some ultralytics versions may not expose .to(); we still move the torch model below.\n",
    "    pass\n",
    "\n",
    "model_torch: nn.Module = yolo.model\n",
    "model_torch = model_torch.to(CFG.device)\n",
    "model_torch.eval()\n",
    "\n",
    "# Keep a reference dtype for safe casting of inputs (MPS is strict about dtype/device).\n",
    "_MODEL_DTYPE = next(model_torch.parameters()).dtype\n",
    "\n",
    "\n",
    "names = getattr(yolo, \"names\", None)\n",
    "if names is None:\n",
    "    names = getattr(model_torch, \"names\", {})\n",
    "\n",
    "if CFG.target_class_name is not None:\n",
    "    target_class_id = _get_class_id_from_names(names, CFG.target_class_name)\n",
    "    if target_class_id is None:\n",
    "        print(f\"[warn] target_class_name='{CFG.target_class_name}' не найден в model.names; использую max по всем классам\")\n",
    "else:\n",
    "    target_class_id = None\n",
    "\n",
    "# ---- Use all pre-head top-level blocks as layers ----\n",
    "def pick_prehead_top_level_blocks(model_torch: nn.Module) -> List[str]:\n",
    "    \"\"\"Return names ['model.0', ..., f'model.{head_start-1}'] for pre-head top-level blocks.\"\"\"\n",
    "    head_start = find_head_start_index(model_torch)\n",
    "    # Confirm model has that many top-level blocks\n",
    "    seq = getattr(model_torch, \"model\", None)\n",
    "    if isinstance(seq, (nn.Sequential, list, tuple)):\n",
    "        n_blocks = len(seq)\n",
    "        head_start = min(head_start, n_blocks)\n",
    "    return [f\"model.{i}\" for i in range(int(head_start))]\n",
    "\n",
    "head_start_idx = find_head_start_index(model_torch)\n",
    "# Choose which blocks to analyze.\n",
    "# - Full pre-head profile: pick_prehead_top_level_blocks(model_torch)\n",
    "# - Single block (requested): [\"model.22\"]\n",
    "layer_names = [\"model.22\"]\n",
    "\n",
    "print(f\"Head starts at top-level model index: {head_start_idx}\")\n",
    "print(f\"Selected blocks: {len(layer_names)} -> {layer_names}\")\n",
    "\n",
    "patch_pil = Image.open(patch_path).convert(\"RGB\")\n",
    "\n",
    "run_data: List[Dict[str, Any]] = []\n",
    "\n",
    "for p in image_paths:\n",
    "    base_pil = Image.open(p).convert(\"RGB\")\n",
    "    clean_lb, patched_lb, patch_bbox_lb = build_clean_and_patched_letterboxed(base_pil, patch_pil, CFG)\n",
    "\n",
    "    conf_clean, res_clean = yolo_predict_conf_scalar(yolo, clean_lb, CFG.imgsz, target_class_id)\n",
    "    conf_patch, res_patch = yolo_predict_conf_scalar(yolo, patched_lb, CFG.imgsz, target_class_id)\n",
    "\n",
    "    gradcam_info = {}\n",
    "    if target_class_id is not None:\n",
    "        gradcam_info = pick_center_person_bbox_from_results(\n",
    "            res_clean, target_class_id=int(target_class_id), imgsz=int(CFG.imgsz)\n",
    "        )\n",
    "\n",
    "    x_clean = torch_preprocess_letterboxed(clean_lb, device=CFG.device, dtype=_MODEL_DTYPE)\n",
    "    x_patch = torch_preprocess_letterboxed(patched_lb, device=CFG.device, dtype=_MODEL_DTYPE)\n",
    "\n",
    "    acts_clean = capture_activations_for_layers(model_torch, x_clean, layer_names)\n",
    "    acts_patch = capture_activations_for_layers(model_torch, x_patch, layer_names)\n",
    "\n",
    "    deltas: Dict[str, torch.Tensor] = {}\n",
    "    for ln in layer_names:\n",
    "        if ln in acts_clean and ln in acts_patch:\n",
    "            deltas[ln] = (acts_patch[ln] - acts_clean[ln])\n",
    "\n",
    "    drop = float(conf_clean - conf_patch)\n",
    "    #success = bool(conf_clean >= CFG.success_thresh > conf_patch)\n",
    "    success = bool(drop > CFG.success_thresh)\n",
    "\n",
    "    run_data.append(\n",
    "        {\n",
    "            \"path\": p,\n",
    "            \"clean_lb\": clean_lb,\n",
    "            \"patched_lb\": patched_lb,\n",
    "            \"patch_bbox_lb\": patch_bbox_lb,\n",
    "            \"conf_clean\": float(conf_clean),\n",
    "            \"conf_patch\": float(conf_patch),\n",
    "            \"drop\": drop,\n",
    "            \"success\": success,\n",
    "            \"acts_clean\": acts_clean,\n",
    "            \"acts_patch\": acts_patch,\n",
    "            \"deltas\": deltas,\n",
    "            \"res_clean\": res_clean,\n",
    "            \"res_patch\": res_patch,\n",
    "            \"gradcam_info\": gradcam_info,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\\nPer-image summary:\")\n",
    "for d in run_data:\n",
    "    print(f\"- {d['path']}: clean={d['conf_clean']:.3f} patched={d['conf_patch']:.3f} drop={d['drop']:.3f} success={d['success']}\")"
   ],
   "id": "3369ba92a89f9b6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Метрики похожести атакуемых фич (Δ) между изображениями\n",
    "\n",
    "Первая часть гипотезы: патч на разных изображениях «бьёт» по похожим фичам.\n",
    "\n",
    "Мы сравниваем `Δ_l` между изображениями несколькими комплементарными метриками:\n",
    "\n",
    "- **cosine(vec(|Δ|))** — сравнение структуры изменения (инвариантность к масштабу, игнорируем знак).\n",
    "- **Spearman по каналам** — совпадает ли ранжирование каналов по `mean|Δ|` (то есть «те же каналы»).\n",
    "- **Jaccard@k по топ-k каналам** — интерпретируемая мера пересечения атакуемых каналов.\n",
    "\n",
    "**Что это даст:**\n",
    "- Понимание, на каких слоях атака ведёт себя «стабильно» между сценами, а где становится контекстно-зависимой."
   ],
   "id": "3b77adfd4ed9a0ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def channel_energy(delta_bchw: torch.Tensor) -> np.ndarray:\n",
    "    return delta_bchw.abs().mean(dim=(0, 2, 3)).numpy()\n",
    "\n",
    "\n",
    "def spatial_energy(delta_bchw: torch.Tensor) -> np.ndarray:\n",
    "    return delta_bchw.abs().sum(dim=1).squeeze(0).numpy()\n",
    "\n",
    "\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    a = a.reshape(-1).astype(np.float64)\n",
    "    b = b.reshape(-1).astype(np.float64)\n",
    "    na = np.linalg.norm(a)\n",
    "    nb = np.linalg.norm(b)\n",
    "    if na < eps or nb < eps:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / (na * nb))\n",
    "\n",
    "\n",
    "def spearman_corr(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    a = a.reshape(-1)\n",
    "    b = b.reshape(-1)\n",
    "    ra = a.argsort().argsort().astype(np.float64)\n",
    "    rb = b.argsort().argsort().astype(np.float64)\n",
    "    ra -= ra.mean()\n",
    "    rb -= rb.mean()\n",
    "    denom = np.linalg.norm(ra) * np.linalg.norm(rb)\n",
    "    if denom < 1e-12:\n",
    "        return 0.0\n",
    "    return float(np.dot(ra, rb) / denom)\n",
    "\n",
    "\n",
    "def jaccard_topk(a: np.ndarray, b: np.ndarray, k: int) -> float:\n",
    "    k = int(min(k, a.size, b.size))\n",
    "    ia = set(np.argsort(-a)[:k].tolist())\n",
    "    ib = set(np.argsort(-b)[:k].tolist())\n",
    "    inter = len(ia & ib)\n",
    "    uni = len(ia | ib)\n",
    "    return float(inter / max(1, uni))\n",
    "\n",
    "\n",
    "def delta_similarity_metrics(delta1: torch.Tensor, delta2: torch.Tensor, topk: int) -> Dict[str, float]:\n",
    "    v1 = delta1.abs().numpy().reshape(-1)\n",
    "    v2 = delta2.abs().numpy().reshape(-1)\n",
    "    cos = cosine_sim(v1, v2)\n",
    "\n",
    "    # Plain Euclidean distance between |Δ| vectors (smaller => more similar)\n",
    "    l2 = float(np.linalg.norm(v1 - v2))\n",
    "\n",
    "    d1 = channel_energy(delta1)\n",
    "    d2 = channel_energy(delta2)\n",
    "    spr = spearman_corr(d1, d2)\n",
    "    jac = jaccard_topk(d1, d2, k=topk)\n",
    "\n",
    "    return {\n",
    "        \"cos_abs\": cos,\n",
    "        \"l2_abs\": l2,\n",
    "        \"spearman_chan\": spr,\n",
    "        \"jaccard_topk\": jac\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Aggregation helpers\n",
    "# -------------------------\n",
    "\n",
    "def summarize(values: List[float]) -> Dict[str, float]:\n",
    "    \"\"\"Return mean and (sample) variance for a list of scalars.\"\"\"\n",
    "    if len(values) == 0:\n",
    "        return {\"mean\": float(\"nan\"), \"var\": float(\"nan\")}\n",
    "    arr = np.asarray(values, dtype=np.float64)\n",
    "    mean = float(arr.mean())\n",
    "    var = float(arr.var(ddof=1)) if arr.size >= 2 else 0.0\n",
    "    return {\"mean\": mean, \"var\": var}\n",
    "\n"
   ],
   "id": "5a797ce5f5ca9830",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Выведем все слои и посмотрим, какой является предпоследним перед головой",
   "id": "b272b577726c2a87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Iterable\n",
    "\n",
    "def list_top_level_blocks(model_torch: nn.Module) -> List[Tuple[int, str, str]]:\n",
    "    \"\"\"Return [(idx, name, class_name)] for top-level blocks model.<idx>.\"\"\"\n",
    "    blocks: List[Tuple[int, str, str]] = []\n",
    "\n",
    "    # Common Ultralytics layout: model_torch has attribute `.model` (nn.Sequential / list-like)\n",
    "    seq = getattr(model_torch, \"model\", None)\n",
    "\n",
    "    if isinstance(seq, (nn.Sequential, list, tuple)):\n",
    "        for i, m in enumerate(seq):\n",
    "            blocks.append((int(i), f\"model.{i}\", m.__class__.__name__))\n",
    "        return blocks\n",
    "\n",
    "    # Fallback: derive from named_modules() by taking unique model.<idx>\n",
    "    seen = set()\n",
    "    for n, m in model_torch.named_modules():\n",
    "        idx = _top_level_model_index(n)\n",
    "        if idx is None:\n",
    "            continue\n",
    "        key = (idx, f\"model.{idx}\")\n",
    "        if key in seen:\n",
    "            continue\n",
    "        # Find the module object for the exact name if possible\n",
    "        try:\n",
    "            mm = get_module_by_name(model_torch, f\"model.{idx}\")\n",
    "            cls = mm.__class__.__name__\n",
    "        except Exception:\n",
    "            cls = m.__class__.__name__\n",
    "        blocks.append((int(idx), f\"model.{idx}\", cls))\n",
    "        seen.add(key)\n",
    "\n",
    "    blocks.sort(key=lambda x: x[0])\n",
    "    return blocks\n",
    "\n",
    "\n",
    "blocks = list_top_level_blocks(model_torch)\n",
    "print(f\"Top-level blocks: {len(blocks)}\")\n",
    "print(\"idx | name     | class\")\n",
    "print(\"----+----------+----------------\")\n",
    "for idx, name, cls in blocks:\n",
    "    mark = \" <== head_start\" if idx == head_start_idx else \"\"\n",
    "    print(f\"{idx:>3} | {name:<8} | {cls}{mark}\")\n",
    "\n",
    "# Extra: show the first module that triggered the head heuristic (by class/name)\n",
    "print(\"\\nHeuristic check (named_modules hits):\")\n",
    "head_classes = {\"Detect\", \"Segment\", \"Pose\", \"OBB\", \"Classify\", \"RTDETRDecoder\", \"WorldDetect\"}\n",
    "for n, m in model_torch.named_modules():\n",
    "    idx = _top_level_model_index(n)\n",
    "    if idx is None:\n",
    "        continue\n",
    "    cls = m.__class__.__name__\n",
    "    nnm = n.lower()\n",
    "    if (cls in head_classes) or (\"detect\" in nnm) or (\"dfl\" in nnm) or (\"decoder\" in nnm):\n",
    "        print(f\"- {n} : {cls}  (top-level idx={idx})\")\n",
    "\n",
    "# Optional: visualize compute graph (may require graphviz/torchviz)\n",
    "# NOTE: Ultralytics may run forward under smart_inference_mode -> inference tensors.\n",
    "# Workaround: build graph from an intermediate activation captured by a hook.\n",
    "try:\n",
    "    from torchviz import make_dot\n",
    "    from IPython.display import display, HTML\n",
    "\n",
    "    # Pick a safe pre-head layer: just before Detect.\n",
    "    # From your printout: Detect starts at model.23, so pre-head is model.22\n",
    "    prehead_name = \"model.22\"\n",
    "    prehead_mod = get_module_by_name(model_torch, prehead_name)\n",
    "\n",
    "    act = {}\n",
    "\n",
    "    def _hook(_m, _inp, out):\n",
    "        # Some blocks output lists/tuples; take first tensor\n",
    "        if isinstance(out, (list, tuple)):\n",
    "            for o in out:\n",
    "                if isinstance(o, torch.Tensor):\n",
    "                    out = o\n",
    "                    break\n",
    "        if isinstance(out, torch.Tensor):\n",
    "            act[\"t\"] = out\n",
    "\n",
    "    h = prehead_mod.register_forward_hook(_hook)\n",
    "\n",
    "    # Temporarily switch to train() to avoid Ultralytics smart_inference_mode on some builds\n",
    "    was_training = model_torch.training\n",
    "    model_torch.train()\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        dummy = torch.zeros(\n",
    "            (1, 3, CFG.imgsz, CFG.imgsz),\n",
    "            device=CFG.device,\n",
    "            dtype=_MODEL_DTYPE,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        _ = model_torch(dummy)\n",
    "\n",
    "    # Restore mode + remove hook\n",
    "    if not was_training:\n",
    "        model_torch.eval()\n",
    "    h.remove()\n",
    "\n",
    "    if \"t\" not in act:\n",
    "        raise RuntimeError(f\"Hook did not capture a tensor from {prehead_name}.\")\n",
    "\n",
    "    t = act[\"t\"]\n",
    "    print(\"Hook layer:\", prehead_name)\n",
    "    print(\"activation.shape =\", tuple(t.shape))\n",
    "    print(\"activation.requires_grad =\", bool(getattr(t, \"requires_grad\", False)))\n",
    "    print(\"activation.is_inference =\", bool(getattr(t, \"is_inference\", False)))\n",
    "\n",
    "    # Root must be scalar\n",
    "    root = t.sum()\n",
    "\n",
    "    dot = make_dot(root, params=dict(model_torch.named_parameters()))\n",
    "    dot.graph_attr.update({\"rankdir\": \"LR\", \"fontsize\": \"10\", \"labelloc\": \"t\"})\n",
    "\n",
    "    dot.format = \"svg\"\n",
    "    svg_path = dot.render(\"yolo_graph_prehead\", cleanup=True)\n",
    "    print(f\"Saved full graph to {svg_path}\")\n",
    "\n",
    "    display(HTML(\n",
    "        f'<div style=\"width:100%;height:900px;overflow:auto;border:1px solid #ddd\">'\n",
    "        f'<object data=\"{svg_path}\" type=\"image/svg+xml\" style=\"width:2400px;height:900px\"></object>'\n",
    "        f'</div>'\n",
    "    ))\n",
    "\n",
    "    dot.format = \"pdf\"\n",
    "    pdf_path = dot.render(\"yolo_graph_prehead\", cleanup=True)\n",
    "    print(f\"Saved full graph PDF to {pdf_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[info] torchviz/graphviz not available or graph rendering failed: {e}\")\n",
    "\n",
    "# Optional: structured model summary (torchinfo)\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "\n",
    "    summary(model_torch, input_size=(1, 3, CFG.imgsz, CFG.imgsz), device=CFG.device)\n",
    "except Exception as e:\n",
    "    print(f\"[info] torchinfo not available: {e}\")"
   ],
   "id": "447fe93882fea7b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "\n",
    "def _pair_type(si: bool, sj: bool) -> str:\n",
    "    if si and sj:\n",
    "        return \"S-S\"\n",
    "    if (si and (not sj)) or ((not si) and sj):\n",
    "        return \"S-F\"\n",
    "    return \"F-F\"\n",
    "\n",
    "def build_pairwise_matrices_for_layer(run_data, layer: str, topk: int):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      labels: image names\n",
    "      mats: metric -> NxN matrix (NaN if missing)\n",
    "      vals_by_type: pairtype -> metric -> list[float]\n",
    "      pairs_by_type: pairtype -> list[(i,j)]\n",
    "    \"\"\"\n",
    "    N = len(run_data)\n",
    "    labels = [Path(d[\"path\"]).name for d in run_data]\n",
    "\n",
    "    metrics = [\"cos_abs\", \"l2_abs\", \"spearman_chan\", \"jaccard_topk\"]\n",
    "    mats = {k: np.full((N, N), np.nan, dtype=np.float64) for k in metrics}\n",
    "\n",
    "    # diagonals\n",
    "    for k in metrics:\n",
    "        mats[k][np.arange(N), np.arange(N)] = 1.0\n",
    "    mats[\"l2_abs\"][np.arange(N), np.arange(N)] = 0.0\n",
    "\n",
    "    vals_by_type = {\n",
    "        \"ALL\": {k: [] for k in metrics},\n",
    "        \"S-S\": {k: [] for k in metrics},\n",
    "        \"S-F\": {k: [] for k in metrics},\n",
    "        \"F-F\": {k: [] for k in metrics},\n",
    "    }\n",
    "    pairs_by_type = {\"ALL\": [], \"S-S\": [], \"S-F\": [], \"F-F\": []}\n",
    "\n",
    "    for i, j in combinations(range(N), 2):\n",
    "        di = run_data[i][\"deltas\"].get(layer, None)\n",
    "        dj = run_data[j][\"deltas\"].get(layer, None)\n",
    "        if di is None or dj is None:\n",
    "            continue\n",
    "\n",
    "        m = delta_similarity_metrics(di, dj, topk=topk)\n",
    "        ptype = _pair_type(bool(run_data[i][\"success\"]), bool(run_data[j][\"success\"]))\n",
    "\n",
    "        for k in metrics:\n",
    "            v = float(m[k])\n",
    "            mats[k][i, j] = v\n",
    "            mats[k][j, i] = v\n",
    "            vals_by_type[\"ALL\"][k].append(v)\n",
    "            vals_by_type[ptype][k].append(v)\n",
    "\n",
    "        pairs_by_type[\"ALL\"].append((i, j))\n",
    "        pairs_by_type[ptype].append((i, j))\n",
    "\n",
    "    return labels, mats, vals_by_type, pairs_by_type\n",
    "\n",
    "\n",
    "def plot_heatmap_with_numbers(\n",
    "    mat: np.ndarray,\n",
    "    xlabels,\n",
    "    ylabels,\n",
    "    title: str,\n",
    "    vmin=None,\n",
    "    vmax=None,\n",
    "    title_fontsize: int = 16,\n",
    "    label_fontsize: int = 10,\n",
    "    number_fontsize: int = 7,\n",
    "    aspect: str = \"equal\",\n",
    "):\n",
    "    \"\"\"Heatmap + numeric annotation. NaNs stay blank. Supports non-square matrices.\"\"\"\n",
    "    H, W = mat.shape\n",
    "    # keep cells readable; grow with matrix size\n",
    "    plt.figure(figsize=(0.45 * W + 5, 0.45 * H + 5))\n",
    "    im = plt.imshow(mat, aspect=aspect, vmin=vmin, vmax=vmax)\n",
    "    plt.title(title, fontsize=title_fontsize)\n",
    "    plt.xticks(np.arange(W), xlabels, rotation=90, fontsize=label_fontsize)\n",
    "    plt.yticks(np.arange(H), ylabels, fontsize=label_fontsize)\n",
    "\n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            v = mat[i, j]\n",
    "            if np.isnan(v):\n",
    "                continue\n",
    "            txt = f\"{v:.2f}\" if abs(v) < 100 else f\"{v:.0f}\"\n",
    "            plt.text(j, i, txt, ha=\"center\", va=\"center\", fontsize=number_fontsize)\n",
    "\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_group_distributions(vals_by_type, metric: str):\n",
    "    \"\"\"Boxplot + mean/var/std for S-S/S-F/F-F.\"\"\"\n",
    "    groups = [\"S-S\", \"S-F\", \"F-F\"]\n",
    "    data = [vals_by_type[g][metric] for g in groups]\n",
    "\n",
    "    print(f\"\\nMetric: {metric}\")\n",
    "    for g in groups:\n",
    "        s = summarize(vals_by_type[g][metric])\n",
    "        std = float(np.sqrt(max(0.0, s[\"var\"])))\n",
    "        print(f\"  {g}: n={len(vals_by_type[g][metric])} mean={s['mean']:.4f} std={std:.4f} var={s['var']:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.boxplot(data, tick_labels=groups, showfliers=True)\n",
    "    plt.title(f\"Pairwise distribution by outcome group: {metric}\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---- RUN (assumes single layer) ----\n",
    "assert len(layer_names) == 1, f\"Expected single layer, got {layer_names}\"\n",
    "layer = layer_names[0]\n",
    "\n",
    "labels, mats, vals_by_type, pairs_by_type = build_pairwise_matrices_for_layer(\n",
    "    run_data, layer=layer, topk=CFG.topk_channels\n",
    ")\n",
    "\n",
    "print(f\"\\nPairwise comparisons for layer: {layer} | N images = {len(labels)}\")\n",
    "print(\"Pairs:\", {k: len(v) for k, v in pairs_by_type.items()})\n",
    "\n",
    "# 1) Full NxN heatmaps\n",
    "plot_heatmap_with_numbers(mats[\"cos_abs\"], labels, labels, f\"{layer} | ALL pairs | cos_abs\", vmin=0.0, vmax=1.0)\n",
    "plot_heatmap_with_numbers(mats[\"spearman_chan\"], labels, labels, f\"{layer} | ALL pairs | spearman_chan\", vmin=-1.0, vmax=1.0)\n",
    "plot_heatmap_with_numbers(mats[\"jaccard_topk\"], labels, labels, f\"{layer} | ALL pairs | jaccard_topk\", vmin=0.0, vmax=1.0)\n",
    "plot_heatmap_with_numbers(mats[\"l2_abs\"], labels, labels, f\"{layer} | ALL pairs | l2_abs\")\n",
    "\n",
    "# 2) Group heatmaps (compact views)\n",
    "metrics = [\"cos_abs\", \"l2_abs\", \"spearman_chan\", \"jaccard_topk\"]\n",
    "\n",
    "succ_idx = [i for i, d in enumerate(run_data) if bool(d[\"success\"])]\n",
    "fail_idx = [i for i, d in enumerate(run_data) if not bool(d[\"success\"])]\n",
    "\n",
    "succ_labels = [labels[i] for i in succ_idx]\n",
    "fail_labels = [labels[i] for i in fail_idx]\n",
    "\n",
    "print(f\"\\nGroup sizes: success={len(succ_idx)} fail={len(fail_idx)}\")\n",
    "\n",
    "for k in metrics:\n",
    "    # S-S: success x success\n",
    "    ss = mats[k][np.ix_(succ_idx, succ_idx)]\n",
    "    title = f\"{layer} | S-S (success-success) | {k}\"\n",
    "    if k in {\"cos_abs\", \"jaccard_topk\"}:\n",
    "        plot_heatmap_with_numbers(ss, succ_labels, succ_labels, title, vmin=0.0, vmax=1.0, title_fontsize=18)\n",
    "    elif k == \"spearman_chan\":\n",
    "        plot_heatmap_with_numbers(ss, succ_labels, succ_labels, title, vmin=-1.0, vmax=1.0, title_fontsize=18)\n",
    "    else:\n",
    "        plot_heatmap_with_numbers(ss, succ_labels, succ_labels, title, title_fontsize=18)\n",
    "\n",
    "    # F-F: fail x fail\n",
    "    ff = mats[k][np.ix_(fail_idx, fail_idx)]\n",
    "    title = f\"{layer} | F-F (fail-fail) | {k}\"\n",
    "    if k in {\"cos_abs\", \"jaccard_topk\"}:\n",
    "        plot_heatmap_with_numbers(ff, fail_labels, fail_labels, title, vmin=0.0, vmax=1.0, title_fontsize=18)\n",
    "    elif k == \"spearman_chan\":\n",
    "        plot_heatmap_with_numbers(ff, fail_labels, fail_labels, title, vmin=-1.0, vmax=1.0, title_fontsize=18)\n",
    "    else:\n",
    "        plot_heatmap_with_numbers(ff, fail_labels, fail_labels, title, title_fontsize=18)\n",
    "\n",
    "    # S-F: success x fail (rectangular)\n",
    "    sf = mats[k][np.ix_(succ_idx, fail_idx)]\n",
    "    title = f\"{layer} | S-F (success-fail) | {k}\"\n",
    "    if k in {\"cos_abs\", \"jaccard_topk\"}:\n",
    "        plot_heatmap_with_numbers(sf, fail_labels, succ_labels, title, vmin=0.0, vmax=1.0, title_fontsize=18, aspect=\"auto\")\n",
    "    elif k == \"spearman_chan\":\n",
    "        plot_heatmap_with_numbers(sf, fail_labels, succ_labels, title, vmin=-1.0, vmax=1.0, title_fontsize=18, aspect=\"auto\")\n",
    "    else:\n",
    "        plot_heatmap_with_numbers(sf, fail_labels, succ_labels, title, title_fontsize=18, aspect=\"auto\")\n",
    "\n",
    "# 3) Distribution comparisons\n",
    "for k in metrics:\n",
    "    plot_group_distributions(vals_by_type, metric=k)"
   ],
   "id": "c8715678ec55c238",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _roi_mask_on_grid(h: int, w: int, stride: float, imgsz: int, bbox_xyxy: Tuple[float, float, float, float], device, dtype):\n",
    "    \"\"\"Create (1,h,w) ROI mask for a bbox in input pixel coords.\n",
    "\n",
    "    We mark a cell as inside ROI if its center lies within bbox. Mask is constant w.r.t. network (OK).\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox_xyxy\n",
    "    ys = (torch.arange(h, device=device, dtype=dtype) + 0.5) * float(stride)\n",
    "    xs = (torch.arange(w, device=device, dtype=dtype) + 0.5) * float(stride)\n",
    "    yy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")\n",
    "\n",
    "    m = (xx >= float(x1)) & (xx <= float(x2)) & (yy >= float(y1)) & (yy <= float(y2))\n",
    "    mask = m.to(dtype=dtype)\n",
    "\n",
    "    # If bbox is tiny on this level, fallback to a single closest cell to bbox center\n",
    "    if float(mask.sum().item()) < 1.0:\n",
    "        cx = 0.5 * (float(x1) + float(x2))\n",
    "        cy = 0.5 * (float(y1) + float(y2))\n",
    "        d2 = (xx - cx) ** 2 + (yy - cy) ** 2\n",
    "        ij = torch.argmin(d2)\n",
    "        mask = torch.zeros_like(mask)\n",
    "        mask.view(-1)[ij] = 1.0\n",
    "\n",
    "    # Normalize to sum=1 for stable scale\n",
    "    mask = mask / (mask.sum() + 1e-12)\n",
    "    return mask.unsqueeze(0)  # (1,h,w)\n",
    "\n",
    "\n",
    "def center_person_surrogate_scalar_bbox(\n",
    "    model_torch: nn.Module,\n",
    "    x_bchw: torch.Tensor,\n",
    "    target_class_id: int,\n",
    "    imgsz: int,\n",
    "    bbox_xyxy: Tuple[float, float, float, float],\n",
    ") -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
    "    \"\"\"Differentiable target scalar: mean person probability *inside the chosen bbox* (selected after NMS).\n",
    "\n",
    "    We bypass Detect._inference() (which may create inference-mode tensors) by forcing Detect head into training path.\n",
    "    \"\"\"\n",
    "    detect = get_module_by_name(model_torch, \"model.23\")\n",
    "\n",
    "    # Keep backbone/neck eval for stability, but Detect train to bypass _inference()\n",
    "    was_detect_training = bool(detect.training)\n",
    "    detect.train()\n",
    "    try:\n",
    "        out = model_torch(x_bchw)\n",
    "    finally:\n",
    "        if not was_detect_training:\n",
    "            detect.eval()\n",
    "\n",
    "    pred_levels = out\n",
    "    if isinstance(out, (tuple, list)) and len(out) == 2 and isinstance(out[0], torch.Tensor):\n",
    "        # some builds return (pred, x)\n",
    "        pred_levels = out[1]\n",
    "\n",
    "    if not isinstance(pred_levels, (list, tuple)) or len(pred_levels) == 0:\n",
    "        raise RuntimeError(f\"Unexpected Detect output type: {type(pred_levels)}\")\n",
    "\n",
    "    reg_max = int(getattr(detect, \"reg_max\", 16))\n",
    "    nc = int(getattr(detect, \"nc\", 80))\n",
    "\n",
    "    strides = getattr(detect, \"strides\", None)\n",
    "    if strides is None:\n",
    "        strides = getattr(detect, \"stride\", None)\n",
    "    if strides is None:\n",
    "        strides = [8, 16, 32][: len(pred_levels)]\n",
    "\n",
    "    if isinstance(strides, torch.Tensor):\n",
    "        strides_list = [float(s) for s in strides.flatten().tolist()]\n",
    "    else:\n",
    "        strides_list = [float(s) for s in list(strides)]\n",
    "\n",
    "    scalars = []\n",
    "    level_debug = []\n",
    "\n",
    "    for li, t in enumerate(pred_levels):\n",
    "        if not (isinstance(t, torch.Tensor) and t.ndim == 4):\n",
    "            continue\n",
    "        B, C, h, w = t.shape\n",
    "        need = reg_max * 4 + nc\n",
    "        if C < need:\n",
    "            raise RuntimeError(f\"Level {li}: unexpected channels C={C}, need >= {need} (reg_max*4+nc)\")\n",
    "\n",
    "        cls_logits = t[:, reg_max * 4: reg_max * 4 + nc, :, :]  # (B,nc,h,w)\n",
    "        # p = cls_logits.sigmoid()[:, int(target_class_id), :, :]  # (B,h,w)\n",
    "        p = cls_logits[:, int(target_class_id), :, :]  # (B,h,w)\n",
    "\n",
    "        # Robust stride estimate from feature map resolution.\n",
    "        # Avoids bugs when pred_levels order doesn't match detect.strides order.\n",
    "        stride_h = float(imgsz) / float(h)\n",
    "        stride_w = float(imgsz) / float(w)\n",
    "        stride = 0.5 * (stride_h + stride_w)\n",
    "        roi = _roi_mask_on_grid(h, w, stride=stride, imgsz=imgsz, bbox_xyxy=bbox_xyxy, device=p.device, dtype=p.dtype)  # (1,h,w)\n",
    "\n",
    "        s = (p * roi).sum(dim=(1, 2))  # (B,)\n",
    "        scalars.append(s)\n",
    "\n",
    "        level_debug.append({\"level\": li, \"shape\": (int(B), int(C), int(h), int(w)), \"stride\": float(stride), \"roi_sum\": float(roi.sum().detach().cpu().item())})\n",
    "\n",
    "    if len(scalars) == 0:\n",
    "        raise RuntimeError(\"No valid prediction tensors from Detect head.\")\n",
    "\n",
    "    scalar = torch.stack(scalars, dim=0).mean(dim=0).mean()  # scalar\n",
    "    info = {\n",
    "        \"levels\": level_debug,\n",
    "        \"reg_max\": reg_max,\n",
    "        \"nc\": nc,\n",
    "        \"target_class_id\": int(target_class_id),\n",
    "        \"bbox_xyxy\": [float(x) for x in bbox_xyxy],\n",
    "    }\n",
    "    return scalar, info\n",
    "\n",
    "\n",
    "def layer_grad_and_act_for_bbox_target(\n",
    "    model_torch: nn.Module,\n",
    "    x_bchw: torch.Tensor,\n",
    "    target_layer_name: str,\n",
    "    target_class_id: int,\n",
    "    imgsz: int,\n",
    "    bbox_xyxy: Tuple[float, float, float, float],\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, float]:\n",
    "    \"\"\"Return (A, G, target_scalar) for the bbox-conditioned person target.\n",
    "\n",
    "    A, G are returned on CPU float32 with shape (1,C,h,w).\n",
    "    \"\"\"\n",
    "    device = next(model_torch.parameters()).device\n",
    "    dtype = next(model_torch.parameters()).dtype\n",
    "\n",
    "    x = x_bchw.to(device=device, dtype=dtype)\n",
    "    x.requires_grad_(True)\n",
    "\n",
    "    layer = get_module_by_name(model_torch, target_layer_name)\n",
    "\n",
    "    buf: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "    def _fwd_hook(_m, _inp, out):\n",
    "        if isinstance(out, (list, tuple)):\n",
    "            for o in out:\n",
    "                if isinstance(o, torch.Tensor):\n",
    "                    out = o\n",
    "                    break\n",
    "        if isinstance(out, torch.Tensor) and out.ndim == 4:\n",
    "            buf[\"A\"] = out\n",
    "            # Ensure grad is retained even if out is not a leaf\n",
    "            try:\n",
    "                out.retain_grad()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Capture gradient flowing into this activation tensor\n",
    "            def _save_grad(g):\n",
    "                if isinstance(g, torch.Tensor) and g.ndim == 4:\n",
    "                    buf[\"G\"] = g\n",
    "                return g\n",
    "\n",
    "            try:\n",
    "                out.register_hook(_save_grad)\n",
    "            except Exception:\n",
    "                # If hooks aren't supported, we'll fall back to .grad after backward (best-effort)\n",
    "                pass\n",
    "\n",
    "\n",
    "    h1 = layer.register_forward_hook(_fwd_hook)\n",
    "\n",
    "    # Save/restore training state for stability\n",
    "    was_training = bool(model_torch.training)\n",
    "\n",
    "\n",
    "    try:\n",
    "        model_torch.zero_grad(set_to_none=True)\n",
    "        # Force autograd-friendly path (avoid inference tensors on some builds)\n",
    "        model_torch.train()\n",
    "\n",
    "\n",
    "        target, _tinfo = center_person_surrogate_scalar_bbox(\n",
    "            model_torch,\n",
    "            x,\n",
    "            target_class_id=int(target_class_id),\n",
    "            imgsz=int(imgsz),\n",
    "            bbox_xyxy=bbox_xyxy,\n",
    "        )\n",
    "        target.backward()\n",
    "\n",
    "        if \"A\" not in buf:\n",
    "            raise RuntimeError(\"Failed to capture activation for the target layer.\")\n",
    "\n",
    "        # Prefer hook-captured grad, else try A.grad\n",
    "        if \"G\" not in buf:\n",
    "            Ag = getattr(buf[\"A\"], \"grad\", None)\n",
    "            if isinstance(Ag, torch.Tensor) and Ag.ndim == 4:\n",
    "                buf[\"G\"] = Ag\n",
    "            else:\n",
    "                raise RuntimeError(\"Failed to capture gradient for the target layer.\")\n",
    "\n",
    "\n",
    "        A = buf[\"A\"].detach().to(device=\"cpu\", dtype=torch.float32)\n",
    "        G = buf[\"G\"].detach().to(device=\"cpu\", dtype=torch.float32)\n",
    "        return A, G, float(target.detach().cpu().item())\n",
    "\n",
    "    finally:\n",
    "        h1.remove()\n",
    "        if not was_training:\n",
    "            model_torch.eval()\n",
    "        x.requires_grad_(False)\n",
    "\n",
    "\n",
    "def _normalize_2d(m: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Normalize (1,1,h,w) or (h,w) to [0,1].\"\"\"\n",
    "    if m.ndim == 2:\n",
    "        m = m.unsqueeze(0).unsqueeze(0)\n",
    "    elif m.ndim == 4:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected ndim for normalize: {m.ndim}\")\n",
    "\n",
    "    m = m - m.amin(dim=(2, 3), keepdim=True)\n",
    "    m = m / m.amax(dim=(2, 3), keepdim=True).clamp(min=1e-12)\n",
    "    return m\n",
    "\n",
    "\n",
    "def gradcam_raw_20(A_bchw: torch.Tensor, G_bchw: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Return Grad-CAM map on the layer grid as (1,1,h,w) in [0,1].\"\"\"\n",
    "    # A,G: (1,C,h,w)\n",
    "    # Standard Grad-CAM weights: global-average-pooled gradients per channel.\n",
    "    w = G_bchw.mean(dim=(2, 3), keepdim=True).detach()  # (1,C,1,1)\n",
    "    cam = (w * A_bchw).sum(dim=1, keepdim=True)         # (1,1,h,w)\n",
    "    cam = F.relu(cam)\n",
    "    # Robust normalization (handle constant maps / NaNs)\n",
    "    cam = torch.nan_to_num(cam, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    cam = _normalize_2d(cam)\n",
    "    return cam\n",
    "\n",
    "\n",
    "def grad_energy_raw_20(G_bchw: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Return |grad| energy map on the layer grid as (1,1,h,w) in [0,1].\"\"\"\n",
    "    m = G_bchw.abs().sum(dim=1, keepdim=True)  # (1,1,h,w)\n",
    "    m = _normalize_2d(m)\n",
    "    return m\n",
    "\n",
    "\n",
    "def delta_energy_raw_20(delta_bchw: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Return |Δ| energy map on the layer grid as (1,1,h,w) in [0,1].\"\"\"\n",
    "    m = delta_bchw.abs().sum(dim=1, keepdim=True)  # (1,1,h,w)\n",
    "    m = _normalize_2d(m)\n",
    "    return m\n",
    "\n",
    "\n",
    "def upsample_map(m_11hw: torch.Tensor, imgsz: int, mode: str) -> np.ndarray:\n",
    "    \"\"\"Upsample (1,1,h,w) -> (imgsz,imgsz) with mode in {'nearest','bilinear'} and return np float32.\"\"\"\n",
    "    if mode == \"nearest\":\n",
    "        up = F.interpolate(m_11hw, size=(imgsz, imgsz), mode=\"nearest\")\n",
    "    elif mode == \"bilinear\":\n",
    "        up = F.interpolate(m_11hw, size=(imgsz, imgsz), mode=\"bilinear\", align_corners=False)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode={mode}\")\n",
    "    return up[0, 0].detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "\n",
    "def show_raw_overlays_side_by_side(\n",
    "    pil_img,\n",
    "    cam_raw_11hw: torch.Tensor,\n",
    "    grad_raw_11hw: torch.Tensor,\n",
    "    delta_raw_11hw: torch.Tensor,\n",
    "    title: str = \"\",\n",
    "    imgsz: int = 640,\n",
    "):\n",
    "    \"\"\"Unsmooth (nearest) overlays side-by-side: Image | Grad-CAM | |∂y/∂A| | |ΔA|.\"\"\"\n",
    "    img = np.asarray(pil_img.convert(\"RGB\"))\n",
    "\n",
    "    def _to_heat(raw_11hw: torch.Tensor) -> np.ndarray:\n",
    "        raw = raw_11hw.detach().to(device=\"cpu\", dtype=torch.float32)\n",
    "        heat = F.interpolate(raw, size=(imgsz, imgsz), mode=\"nearest\")[0, 0].numpy()\n",
    "        return heat\n",
    "\n",
    "    cam_h = _to_heat(cam_raw_11hw)\n",
    "    grad_h = _to_heat(grad_raw_11hw)\n",
    "    delta_h = _to_heat(delta_raw_11hw)\n",
    "\n",
    "    plt.figure(figsize=(22, 5))\n",
    "\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(cam_h, alpha=0.45, vmin=0.0, vmax=1.0)\n",
    "    plt.title(\"Grad-CAM | raw 20×20 (nearest)\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(delta_h, alpha=0.45, vmin=0.0, vmax=1.0)\n",
    "    plt.title(\"|ΔA| energy | raw 20×20 (nearest)\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(grad_h, alpha=0.45, vmin=0.0, vmax=1.0)\n",
    "    plt.title(\"|∂y/∂A| energy | raw 20×20 (nearest)\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    if title:\n",
    "        plt.suptitle(title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "e37ef804c56d5793",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---- RUN: pick a few examples and show raw(20x20) vs upsample variants ----\n",
    "TARGET_LAYER = \"model.22\"\n",
    "N_EX = 5\n",
    "\n",
    "examples = []\n",
    "\n",
    "# mix of success/fail if available\n",
    "succ = [d for d in run_data if bool(d[\"success\"])]\n",
    "fail = [d for d in run_data if (not bool(d[\"success\"]))]\n",
    "examples = (succ[:N_EX] + fail[:N_EX])\n",
    "\n",
    "for d in examples:\n",
    "    info = d.get(\"gradcam_info\", {})\n",
    "    bbox = info.get(\"picked_bbox\", None)\n",
    "    if bbox is None:\n",
    "        continue\n",
    "\n",
    "    delta = d.get(\"deltas\", {}).get(TARGET_LAYER, None)\n",
    "    if delta is None:\n",
    "        continue\n",
    "\n",
    "    x_clean = torch_preprocess_letterboxed(d[\"clean_lb\"], device=CFG.device, dtype=_MODEL_DTYPE)\n",
    "    A, G, tgt = layer_grad_and_act_for_bbox_target(\n",
    "        model_torch,\n",
    "        x_clean,\n",
    "        target_layer_name=TARGET_LAYER,\n",
    "        target_class_id=int(target_class_id),\n",
    "        imgsz=int(CFG.imgsz),\n",
    "        bbox_xyxy=tuple(float(x) for x in bbox),\n",
    "    )\n",
    "    w_dbg = G.mean(dim=(2, 3))  # (1,C)\n",
    "    print(\n",
    "        \"[debug]\", Path(d[\"path\"]).name,\n",
    "        \"A|mean|\", float(A.abs().mean()),\n",
    "        \"G|mean|\", float(G.abs().mean()),\n",
    "        \"G|max|\", float(G.abs().max()),\n",
    "        \"w|mean|\", float(w_dbg.abs().mean()),\n",
    "        \"w|pos_frac|\", float((w_dbg > 0).float().mean()),\n",
    "    )\n",
    "\n",
    "    # raw maps on layer grid (1,1,20,20)\n",
    "    cam_raw = gradcam_raw_20(A, G)\n",
    "    grad_raw = grad_energy_raw_20(G)\n",
    "    delta_raw = delta_energy_raw_20(delta)\n",
    "\n",
    "    base_title = f\"{Path(d['path']).name} | target={tgt:.3f} | picked_conf={info.get('picked_conf', float('nan')):.3f}\"\n",
    "\n",
    "    show_raw_overlays_side_by_side(\n",
    "        d[\"clean_lb\"],\n",
    "        cam_raw,\n",
    "        grad_raw,\n",
    "        delta_raw,\n",
    "        title=base_title,\n",
    "        imgsz=int(CFG.imgsz),\n",
    "    )"
   ],
   "id": "6922e5bcac701be1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Tuple, Optional\n",
    "\n",
    "# -------------------------\n",
    "# SSGrad-CAM target variants\n",
    "# -------------------------\n",
    "\n",
    "def _detect_forward_train_levels(model_torch: nn.Module, x_bchw: torch.Tensor):\n",
    "    \"\"\"Run model forward with Detect head forced to train() to avoid inference tensors. Returns pred_levels and detect module.\"\"\"\n",
    "    detect = get_module_by_name(model_torch, \"model.23\")\n",
    "    was_detect_training = bool(detect.training)\n",
    "    detect.train()\n",
    "    try:\n",
    "        out = model_torch(x_bchw)\n",
    "    finally:\n",
    "        if not was_detect_training:\n",
    "            detect.eval()\n",
    "\n",
    "    pred_levels = out\n",
    "    if isinstance(out, (tuple, list)) and len(out) == 2 and isinstance(out[0], torch.Tensor):\n",
    "        pred_levels = out[1]\n",
    "    if not isinstance(pred_levels, (list, tuple)) or len(pred_levels) == 0:\n",
    "        raise RuntimeError(f\"Unexpected Detect output type: {type(pred_levels)}\")\n",
    "\n",
    "    reg_max = int(getattr(detect, \"reg_max\", 16))\n",
    "    nc = int(getattr(detect, \"nc\", 80))\n",
    "    return pred_levels, detect, reg_max, nc\n",
    "\n",
    "\n",
    "def _person_logit_map_from_level(t: torch.Tensor, reg_max: int, nc: int, target_class_id: int) -> torch.Tensor:\n",
    "    \"\"\"t: (B,C,h,w) -> person logits (B,h,w).\"\"\"\n",
    "    cls_logits = t[:, reg_max * 4: reg_max * 4 + nc, :, :]\n",
    "    return cls_logits[:, int(target_class_id), :, :]\n",
    "\n",
    "\n",
    "def _roi_mask_bin(h: int, w: int, imgsz: int, bbox_xyxy, device, dtype) -> torch.Tensor:\n",
    "    # stride estimate from resolution\n",
    "    stride = 0.5 * (float(imgsz) / float(h) + float(imgsz) / float(w))\n",
    "    roi = _roi_mask_on_grid(h, w, stride=stride, imgsz=imgsz, bbox_xyxy=bbox_xyxy, device=device, dtype=dtype)  # (1,h,w) sum=1\n",
    "    roi_bin = (roi > 0).to(dtype=dtype)  # (1,h,w)\n",
    "    return roi_bin\n",
    "\n",
    "\n",
    "def detector_target_scalar(\n",
    "    model_torch: nn.Module,\n",
    "    x_bchw: torch.Tensor,\n",
    "    target_class_id: int,\n",
    "    imgsz: int,\n",
    "    mode: str,\n",
    "    bbox_xyxy: Optional[Tuple[float, float, float, float]] = None,\n",
    "    topk: int = 10,\n",
    "    thr: float = 0.30,\n",
    ") -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
    "    \"\"\"Compute scalar y for a given mode. Returns (y, info).\"\"\"\n",
    "    pred_levels, detect, reg_max, nc = _detect_forward_train_levels(model_torch, x_bchw)\n",
    "\n",
    "    dbg = {\"mode\": mode, \"levels\": [], \"topk\": int(topk), \"thr\": float(thr)}\n",
    "\n",
    "    if mode == \"global_max_prob\":\n",
    "        # y = max sigmoid(person_logit) over all levels/cells\n",
    "        vals = []\n",
    "        for li, t in enumerate(pred_levels):\n",
    "            if not (isinstance(t, torch.Tensor) and t.ndim == 4):\n",
    "                continue\n",
    "            pl = _person_logit_map_from_level(t, reg_max, nc, target_class_id)  # (B,h,w)\n",
    "            p = pl.sigmoid()\n",
    "            v = p.reshape(p.shape[0], -1).max(dim=1).values  # (B,)\n",
    "            vals.append(v)\n",
    "            dbg[\"levels\"].append({\"level\": li, \"shape\": tuple(t.shape), \"p_max\": float(v.detach().mean().cpu().item())})\n",
    "        y = torch.stack(vals, dim=0).amax(dim=0).mean()\n",
    "        return y, dbg\n",
    "\n",
    "    # Modes below require bbox\n",
    "    if bbox_xyxy is None:\n",
    "        raise ValueError(f\"mode='{mode}' requires bbox_xyxy\")\n",
    "\n",
    "    if mode == \"avg_roi_logits\":\n",
    "        # current variant: mean person logits inside ROI mask on each level, then average over levels\n",
    "        scalars = []\n",
    "        for li, t in enumerate(pred_levels):\n",
    "            if not (isinstance(t, torch.Tensor) and t.ndim == 4):\n",
    "                continue\n",
    "            B, C, h, w = t.shape\n",
    "            pl = _person_logit_map_from_level(t, reg_max, nc, target_class_id)  # (B,h,w)\n",
    "            roi = _roi_mask_on_grid(h, w, stride=0.5*(float(imgsz)/h + float(imgsz)/w), imgsz=imgsz,\n",
    "                                    bbox_xyxy=bbox_xyxy, device=pl.device, dtype=pl.dtype)  # (1,h,w) sum=1\n",
    "            s = (pl * roi).sum(dim=(1,2))  # (B,)\n",
    "            scalars.append(s)\n",
    "            dbg[\"levels\"].append({\"level\": li, \"shape\": tuple(t.shape), \"roi_cells\": int((roi>0).sum().detach().cpu().item())})\n",
    "        y = torch.stack(scalars, dim=0).mean(dim=0).mean()\n",
    "        return y, dbg\n",
    "\n",
    "    if mode == \"topk10_roi_logits\":\n",
    "        # rank by prob inside ROI across ALL levels, but aggregate logits of selected cells\n",
    "        all_logits = []\n",
    "        all_probs = []\n",
    "        for li, t in enumerate(pred_levels):\n",
    "            if not (isinstance(t, torch.Tensor) and t.ndim == 4):\n",
    "                continue\n",
    "            pl = _person_logit_map_from_level(t, reg_max, nc, target_class_id)  # (B,h,w)\n",
    "            roi_bin = _roi_mask_bin(pl.shape[1], pl.shape[2], imgsz, bbox_xyxy, device=pl.device, dtype=pl.dtype)  # (1,h,w)\n",
    "            logits_roi = pl * roi_bin  # zeros outside\n",
    "\n",
    "            # flatten only ROI cells\n",
    "            mask = roi_bin[0].reshape(-1) > 0\n",
    "            if mask.sum().item() < 1:\n",
    "                continue\n",
    "            lr = pl.reshape(pl.shape[0], -1)[:, mask]  # (B, n_roi)\n",
    "            pr = lr.sigmoid()\n",
    "            all_logits.append(lr)\n",
    "            all_probs.append(pr)\n",
    "            dbg[\"levels\"].append({\"level\": li, \"shape\": tuple(t.shape), \"roi_n\": int(mask.sum().detach().cpu().item())})\n",
    "\n",
    "        if len(all_logits) == 0:\n",
    "            # fallback: max prob in bbox using avg mode machinery\n",
    "            y, _ = detector_target_scalar(model_torch, x_bchw, target_class_id, imgsz, mode=\"roi_prob_gt_thr_logits\",\n",
    "                                          bbox_xyxy=bbox_xyxy, topk=topk, thr=-1.0)\n",
    "            dbg[\"fallback\"] = \"no_roi_cells\"\n",
    "            return y, dbg\n",
    "\n",
    "        L = torch.cat(all_logits, dim=1)  # (B, total_roi)\n",
    "        P = torch.cat(all_probs, dim=1)   # (B, total_roi)\n",
    "\n",
    "        k = int(min(topk, P.shape[1]))\n",
    "        idx = torch.topk(P, k=k, dim=1, largest=True, sorted=False).indices  # (B,k)\n",
    "        chosen_logits = torch.gather(L, dim=1, index=idx)  # (B,k)\n",
    "        y = chosen_logits.mean(dim=1).mean()  # scalar\n",
    "        dbg[\"chosen_k\"] = int(k)\n",
    "        return y, dbg\n",
    "\n",
    "    if mode == \"roi_prob_gt_thr_logits\":\n",
    "        # include ALL ROI cells with prob > thr, aggregate logits (fallback to max logit in ROI)\n",
    "        vals = []\n",
    "        n_keep = []\n",
    "        for li, t in enumerate(pred_levels):\n",
    "            if not (isinstance(t, torch.Tensor) and t.ndim == 4):\n",
    "                continue\n",
    "            pl = _person_logit_map_from_level(t, reg_max, nc, target_class_id)  # (B,h,w)\n",
    "            roi_bin = _roi_mask_bin(pl.shape[1], pl.shape[2], imgsz, bbox_xyxy, device=pl.device, dtype=pl.dtype)  # (1,h,w)\n",
    "            pr = pl.sigmoid()\n",
    "            keep = (roi_bin > 0) & (pr > float(thr))  # (1,h,w)\n",
    "\n",
    "            kb = keep.reshape(keep.shape[0], -1)  # (1, h*w)\n",
    "            lb = pl.reshape(pl.shape[0], -1)\n",
    "\n",
    "            if kb.sum().item() < 1:\n",
    "                # fallback to max logit inside ROI\n",
    "                roi_mask = (roi_bin.reshape(1, -1) > 0)\n",
    "                lb_roi = lb.masked_fill(~roi_mask, -1e9)\n",
    "                v = lb_roi.max(dim=1).values\n",
    "                vals.append(v)\n",
    "                n_keep.append(0)\n",
    "            else:\n",
    "                # mean of logits over kept cells\n",
    "                lb_keep = lb.masked_select(kb).reshape(pl.shape[0], -1)\n",
    "                v = lb_keep.mean(dim=1)\n",
    "                vals.append(v)\n",
    "                n_keep.append(int(kb.sum().detach().cpu().item()))\n",
    "\n",
    "            dbg[\"levels\"].append({\"level\": li, \"shape\": tuple(t.shape), \"n_keep\": int(n_keep[-1])})\n",
    "\n",
    "        y = torch.stack(vals, dim=0).mean(dim=0).mean()  # scalar\n",
    "        dbg[\"n_keep_total\"] = int(sum(n_keep))\n",
    "        return y, dbg\n",
    "\n",
    "    raise ValueError(f\"Unknown mode='{mode}'\")\n",
    "\n",
    "\n",
    "def layer_grad_and_act_for_detector_target(\n",
    "    model_torch: nn.Module,\n",
    "    x_bchw: torch.Tensor,\n",
    "    target_layer_name: str,\n",
    "    target_class_id: int,\n",
    "    imgsz: int,\n",
    "    mode: str,\n",
    "    bbox_xyxy: Optional[Tuple[float, float, float, float]] = None,\n",
    "    topk: int = 10,\n",
    "    thr: float = 0.30,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, float, Dict[str, Any]]:\n",
    "    \"\"\"Return (A, G, y_scalar_float, y_info) for a given detector target mode.\"\"\"\n",
    "    device = next(model_torch.parameters()).device\n",
    "    dtype = next(model_torch.parameters()).dtype\n",
    "\n",
    "    x = x_bchw.to(device=device, dtype=dtype)\n",
    "    x.requires_grad_(True)\n",
    "\n",
    "    layer = get_module_by_name(model_torch, target_layer_name)\n",
    "    buf: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "    def _fwd_hook(_m, _inp, out):\n",
    "        if isinstance(out, (list, tuple)):\n",
    "            for o in out:\n",
    "                if isinstance(o, torch.Tensor):\n",
    "                    out = o\n",
    "                    break\n",
    "        if isinstance(out, torch.Tensor) and out.ndim == 4:\n",
    "            buf[\"A\"] = out\n",
    "            try:\n",
    "                out.retain_grad()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            def _save_grad(g):\n",
    "                if isinstance(g, torch.Tensor) and g.ndim == 4:\n",
    "                    buf[\"G\"] = g\n",
    "                return g\n",
    "\n",
    "            try:\n",
    "                out.register_hook(_save_grad)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    h = layer.register_forward_hook(_fwd_hook)\n",
    "    was_training = bool(model_torch.training)\n",
    "\n",
    "    try:\n",
    "        model_torch.zero_grad(set_to_none=True)\n",
    "        model_torch.train()  # make sure forward path is autograd-friendly\n",
    "\n",
    "        y, info = detector_target_scalar(\n",
    "            model_torch,\n",
    "            x,\n",
    "            target_class_id=int(target_class_id),\n",
    "            imgsz=int(imgsz),\n",
    "            mode=str(mode),\n",
    "            bbox_xyxy=bbox_xyxy,\n",
    "            topk=int(topk),\n",
    "            thr=float(thr),\n",
    "        )\n",
    "        y.backward()\n",
    "\n",
    "        if \"A\" not in buf:\n",
    "            raise RuntimeError(\"Failed to capture activation A for the target layer.\")\n",
    "        if \"G\" not in buf:\n",
    "            Ag = getattr(buf[\"A\"], \"grad\", None)\n",
    "            if isinstance(Ag, torch.Tensor) and Ag.ndim == 4:\n",
    "                buf[\"G\"] = Ag\n",
    "            else:\n",
    "                raise RuntimeError(\"Failed to capture gradient G for the target layer.\")\n",
    "\n",
    "        A = buf[\"A\"].detach().to(device=\"cpu\", dtype=torch.float32)\n",
    "        G = buf[\"G\"].detach().to(device=\"cpu\", dtype=torch.float32)\n",
    "        return A, G, float(y.detach().cpu().item()), info\n",
    "\n",
    "    finally:\n",
    "        h.remove()\n",
    "        if not was_training:\n",
    "            model_torch.eval()\n",
    "        x.requires_grad_(False)\n",
    "\n",
    "\n",
    "def ssgradcam_raw(A_bchw: torch.Tensor, G_bchw: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
    "    # channel weights (1,C,1,1)\n",
    "    w = G_bchw.mean(dim=(2, 3), keepdim=True)\n",
    "    # space maps (1,C,H,W) in [0,1]\n",
    "    S = G_bchw.abs()\n",
    "    S = S / (S.amax(dim=(2, 3), keepdim=True).clamp(min=eps))\n",
    "    cam = ((w * A_bchw) * S).sum(dim=1, keepdim=True)\n",
    "    cam = F.relu(cam)\n",
    "    cam = torch.nan_to_num(cam, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    cam = cam - cam.amin(dim=(2, 3), keepdim=True)\n",
    "    cam = cam / cam.amax(dim=(2, 3), keepdim=True).clamp(min=eps)\n",
    "    return cam\n",
    "\n",
    "\n",
    "def _overlay_four(pil_img, maps_11hw, titles, suptitle=\"\", imgsz=640):\n",
    "    img = np.asarray(pil_img.convert(\"RGB\"))\n",
    "\n",
    "    def up(h):\n",
    "        return F.interpolate(h.detach().cpu(), size=(imgsz, imgsz), mode=\"nearest\")[0,0].numpy()\n",
    "\n",
    "    hs = [up(m) for m in maps_11hw]\n",
    "\n",
    "    plt.figure(figsize=(22,5))\n",
    "    for i in range(4):\n",
    "        plt.subplot(1,4,i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(hs[i], alpha=0.45, vmin=0.0, vmax=1.0)\n",
    "        plt.title(titles[i])\n",
    "        plt.axis(\"off\")\n",
    "    if suptitle:\n",
    "        plt.suptitle(suptitle)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---- RUN: compare SSGrad-CAM across y definitions ----\n",
    "TARGET_LAYER = \"model.22\"\n",
    "N_EX = 10\n",
    "TOPK = 3\n",
    "THR = 0.30\n",
    "\n",
    "modes = [\n",
    "    \"avg_roi_logits\",\n",
    "    \"global_max_prob\",\n",
    "    \"topk10_roi_logits\",\n",
    "    \"roi_prob_gt_thr_logits\",\n",
    "]\n",
    "\n",
    "succ = [d for d in run_data if bool(d[\"success\"])]\n",
    "fail = [d for d in run_data if (not bool(d[\"success\"]))]\n",
    "examples = (succ[:N_EX] + fail[:N_EX])\n",
    "\n",
    "for d in examples:\n",
    "    info = d.get(\"gradcam_info\", {})\n",
    "    bbox = info.get(\"picked_bbox\", None)\n",
    "    if bbox is None:\n",
    "        continue\n",
    "\n",
    "    x_clean = torch_preprocess_letterboxed(d[\"clean_lb\"], device=CFG.device, dtype=_MODEL_DTYPE)\n",
    "\n",
    "    maps = []\n",
    "    titles = []\n",
    "    y_dbg = []\n",
    "\n",
    "    for mode in modes:\n",
    "        bb = None if mode == \"global_max_prob\" else tuple(float(v) for v in bbox)\n",
    "        A, G, y, yinfo = layer_grad_and_act_for_detector_target(\n",
    "            model_torch,\n",
    "            x_clean,\n",
    "            target_layer_name=TARGET_LAYER,\n",
    "            target_class_id=int(target_class_id),\n",
    "            imgsz=int(CFG.imgsz),\n",
    "            mode=mode,\n",
    "            bbox_xyxy=bb,\n",
    "            topk=int(TOPK),\n",
    "            thr=float(THR),\n",
    "        )\n",
    "        ssg = ssgradcam_raw(A, G)\n",
    "        maps.append(ssg)\n",
    "\n",
    "        if mode == \"avg_roi_logits\":\n",
    "            titles.append(f\"avg ROI logits\\ny={y:.3f}\")\n",
    "        elif mode == \"global_max_prob\":\n",
    "            titles.append(f\"global max prob\\ny={y:.3f}\")\n",
    "        elif mode == \"topk10_roi_logits\":\n",
    "            titles.append(f\"top-{TOPK} in ROI (logits)\\ny={y:.3f}\")\n",
    "        else:\n",
    "            nk = yinfo.get(\"n_keep_total\", None)\n",
    "            titles.append(f\"ROI p>{THR} (logits)\\ny={y:.3f}\\nkeep={nk}\")\n",
    "\n",
    "        y_dbg.append({\"mode\": mode, \"y\": y, \"extra\": {k:v for k,v in yinfo.items() if k in (\"n_keep_total\",\"chosen_k\")}})\n",
    "\n",
    "    base_title = (\n",
    "        f\"{Path(d['path']).name} | success={bool(d['success'])} | drop={float(d['drop']):.3f} | \"\n",
    "        f\"picked_conf={float(info.get('picked_conf', np.nan)):.3f}\"\n",
    "    )\n",
    "\n",
    "    _overlay_four(d[\"clean_lb\"], maps, titles, suptitle=base_title, imgsz=int(CFG.imgsz))\n"
   ],
   "id": "2382795c8cc55891",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Численное сравнение: совпадение «важных» и «атакованных» фич\n",
    "\n",
    "Считаем метрики между:\n",
    "- I — картой важности (20×20) на слое model.22\n",
    "- D — картой атакованных фич |ΔA| (20×20)\n",
    "\n",
    "Цель: найти метрики, которые разделяют success vs fail.\n",
    "Для каждой метрики рисуем boxplot и ROC/AUC.\n",
    "Все метрики считаем на raw 20×20 без сглаживания."
   ],
   "id": "24a2cfe2a2115690"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _to_np_20(raw_11hw: torch.Tensor) -> np.ndarray:\n",
    "    return raw_11hw[0, 0].detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "def _normalize_np(m: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    m = m.astype(np.float32)\n",
    "    mn, mx = float(m.min()), float(m.max())\n",
    "    if mx - mn < eps:\n",
    "        return np.zeros_like(m, dtype=np.float32)\n",
    "    return (m - mn) / (mx - mn)\n",
    "\n",
    "def _flatten(m: np.ndarray) -> np.ndarray:\n",
    "    return m.reshape(-1).astype(np.float32)\n",
    "\n",
    "def topk_mask(m: np.ndarray, k: int) -> np.ndarray:\n",
    "    v = _flatten(m)\n",
    "    k = int(min(k, v.size))\n",
    "    if k <= 0:\n",
    "        return np.zeros_like(m, dtype=bool)\n",
    "    idx = np.argpartition(-v, k - 1)[:k]\n",
    "    mask = np.zeros(v.size, dtype=bool)\n",
    "    mask[idx] = True\n",
    "    return mask.reshape(m.shape)\n",
    "\n",
    "def iou_binary(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    inter = float(np.logical_and(a, b).sum())\n",
    "    uni = float(np.logical_or(a, b).sum())\n",
    "    return float(inter / max(1.0, uni))\n",
    "\n",
    "def weighted_jaccard(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    a = np.clip(a, 0.0, None)\n",
    "    b = np.clip(b, 0.0, None)\n",
    "    num = float(np.minimum(a, b).sum())\n",
    "    den = float(np.maximum(a, b).sum())\n",
    "    return float(num / max(eps, den))\n",
    "\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    a = _flatten(a).astype(np.float64)\n",
    "    b = _flatten(b).astype(np.float64)\n",
    "    na, nb = float(np.linalg.norm(a)), float(np.linalg.norm(b))\n",
    "    if na < eps or nb < eps:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / (na * nb))\n",
    "\n",
    "def pearson_corr(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    a = _flatten(a).astype(np.float64); b = _flatten(b).astype(np.float64)\n",
    "    a -= a.mean(); b -= b.mean()\n",
    "    den = float(np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    if den < eps:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / den)\n",
    "\n",
    "def spearman_corr(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    a = _flatten(a); b = _flatten(b)\n",
    "    ra = a.argsort().argsort().astype(np.float64)\n",
    "    rb = b.argsort().argsort().astype(np.float64)\n",
    "    ra -= ra.mean(); rb -= rb.mean()\n",
    "    den = float(np.linalg.norm(ra) * np.linalg.norm(rb))\n",
    "    if den < eps:\n",
    "        return 0.0\n",
    "    return float(np.dot(ra, rb) / den)\n",
    "\n",
    "def energy_overlap_fraction(D: np.ndarray, I: np.ndarray, top_frac: float = 0.2) -> float:\n",
    "    v = _flatten(I)\n",
    "    k = int(max(1, round(v.size * float(top_frac))))\n",
    "    m = topk_mask(I, k)\n",
    "    num = float(D[m].sum())\n",
    "    den = float(D.sum())\n",
    "    return float(num / max(1e-12, den))\n",
    "\n",
    "def auc_roc(y_true: np.ndarray, scores: np.ndarray) -> float:\n",
    "    y = y_true.astype(np.int32)\n",
    "    s = scores.astype(np.float64)\n",
    "    n_pos = int((y == 1).sum()); n_neg = int((y == 0).sum())\n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        return float(\"nan\")\n",
    "    order = np.argsort(s)\n",
    "    ranks = np.empty_like(order, dtype=np.float64)\n",
    "    ranks[order] = np.arange(1, len(s) + 1, dtype=np.float64)\n",
    "    # average ranks for ties\n",
    "    s_sorted = s[order]\n",
    "    i = 0\n",
    "    while i < len(s_sorted):\n",
    "        j = i + 1\n",
    "        while j < len(s_sorted) and s_sorted[j] == s_sorted[i]:\n",
    "            j += 1\n",
    "        if j - i > 1:\n",
    "            avg = ranks[order[i:j]].mean()\n",
    "            ranks[order[i:j]] = avg\n",
    "        i = j\n",
    "    sum_ranks_pos = float(ranks[y == 1].sum())\n",
    "    auc = (sum_ranks_pos - n_pos * (n_pos + 1) / 2.0) / (n_pos * n_neg)\n",
    "    return float(auc)\n",
    "\n",
    "def roc_curve_points(y_true: np.ndarray, scores: np.ndarray):\n",
    "    y = y_true.astype(np.int32)\n",
    "    s = scores.astype(np.float64)\n",
    "    order = np.argsort(-s)\n",
    "    y = y[order]; s = s[order]\n",
    "    P = float((y == 1).sum()); N = float((y == 0).sum())\n",
    "    if P == 0 or N == 0:\n",
    "        return np.array([0.0, 1.0]), np.array([0.0, 1.0])\n",
    "    tp = fp = 0.0\n",
    "    fpr = [0.0]; tpr = [0.0]\n",
    "    last_s = None\n",
    "    for yi, si in zip(y, s):\n",
    "        if last_s is None:\n",
    "            last_s = si\n",
    "        if si != last_s:\n",
    "            fpr.append(fp / N); tpr.append(tp / P)\n",
    "            last_s = si\n",
    "        if yi == 1: tp += 1.0\n",
    "        else: fp += 1.0\n",
    "    fpr.append(fp / N); tpr.append(tp / P)\n",
    "    return np.asarray(fpr), np.asarray(tpr)\n",
    "\n",
    "def boxplot_metric(vals_fail, vals_succ, title, ylabel):\n",
    "    plt.figure(figsize=(6.2, 4.0))\n",
    "    plt.boxplot([vals_fail, vals_succ], tick_labels=[\"fail\", \"success\"], showfliers=True)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def rocplot_metric(y, s, title):\n",
    "    auc = auc_roc(y, s)\n",
    "    fpr, tpr = roc_curve_points(y, s)\n",
    "    plt.figure(figsize=(5.2, 4.6))\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.title(f\"{title}\\nAUC={auc:.3f}\")\n",
    "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- SSGrad-CAM with top-3 in ROI (logits) target ---\n",
    "def ssgradcam_raw_20(A_bchw: torch.Tensor, G_bchw: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
    "    \"\"\"SSGrad-CAM on the layer grid as (1,1,h,w) in [0,1].\"\"\"\n",
    "    # channel weights\n",
    "    w = G_bchw.mean(dim=(2, 3), keepdim=True)  # (1,C,1,1)\n",
    "    # spatial sensitivity maps per channel\n",
    "    S = G_bchw.abs()\n",
    "    S = S / (S.amax(dim=(2, 3), keepdim=True).clamp(min=eps))  # (1,C,h,w) in [0,1]\n",
    "    cam = ((w * A_bchw) * S).sum(dim=1, keepdim=True)  # (1,1,h,w)\n",
    "    cam = F.relu(cam)\n",
    "    cam = torch.nan_to_num(cam, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    cam = _normalize_2d(cam)\n",
    "    return cam\n",
    "\n",
    "# ---------- build per-image metrics ----------\n",
    "TARGET_LAYER = \"model.22\"\n",
    "TOPK_CELLS = 20     # out of 400\n",
    "TOP_FRAC = 0.20\n",
    "\n",
    "rows = []\n",
    "\n",
    "for d in run_data:\n",
    "    info = d.get(\"gradcam_info\", {})\n",
    "    bbox = info.get(\"picked_bbox\", None)\n",
    "    if bbox is None:\n",
    "        continue\n",
    "\n",
    "    delta = d.get(\"deltas\", {}).get(TARGET_LAYER, None)\n",
    "    if delta is None:\n",
    "        continue\n",
    "\n",
    "    x_clean = torch_preprocess_letterboxed(d[\"clean_lb\"], device=CFG.device, dtype=_MODEL_DTYPE)\n",
    "    A, G, tgt = layer_grad_and_act_for_bbox_target(\n",
    "        model_torch,\n",
    "        x_clean,\n",
    "        target_layer_name=TARGET_LAYER,\n",
    "        target_class_id=int(target_class_id),\n",
    "        imgsz=int(CFG.imgsz),\n",
    "        bbox_xyxy=tuple(float(x) for x in bbox),\n",
    "    )\n",
    "\n",
    "    # SSGrad-CAM with y = mean(logits) over top-3 person predictions inside bbox (rank by sigmoid)\n",
    "    A3, G3, y3, _y3info = layer_grad_and_act_for_detector_target(\n",
    "        model_torch,\n",
    "        x_clean,\n",
    "        target_layer_name=TARGET_LAYER,\n",
    "        target_class_id=int(target_class_id),\n",
    "        imgsz=int(CFG.imgsz),\n",
    "        mode=\"topk10_roi_logits\",\n",
    "        bbox_xyxy=tuple(float(x) for x in bbox),\n",
    "        topk=3,\n",
    "        thr=0.30,\n",
    "    )\n",
    "\n",
    "    # importance maps (raw 20×20)\n",
    "    I_g  = grad_energy_raw_20(G)                        # (1,1,20,20)\n",
    "    I_gxa = _normalize_2d((G * A).abs().sum(dim=1, keepdim=True))  # (1,1,20,20)\n",
    "    I_ssg3 = ssgradcam_raw_20(A3, G3)\n",
    "\n",
    "    # attack map (raw 20×20)\n",
    "    D = delta_energy_raw_20(delta)                      # (1,1,20,20)\n",
    "\n",
    "    # to numpy + normalize\n",
    "    Dn = _normalize_np(_to_np_20(D))\n",
    "    mkD = topk_mask(Dn, TOPK_CELLS)\n",
    "\n",
    "    def compute(Iraw: torch.Tensor):\n",
    "        In = _normalize_np(_to_np_20(Iraw))\n",
    "        mkI = topk_mask(In, TOPK_CELLS)\n",
    "        return {\n",
    "            \"iou_topk\": iou_binary(mkI, mkD),\n",
    "            \"cos\": cosine_sim(In, Dn),\n",
    "            \"pearson\": pearson_corr(In, Dn),\n",
    "            \"spearman\": spearman_corr(In, Dn),\n",
    "            \"delta_in_topI\": energy_overlap_fraction(Dn, In, top_frac=TOP_FRAC),\n",
    "        }\n",
    "\n",
    "    m_g   = compute(I_g)\n",
    "    m_gxa = compute(I_gxa)\n",
    "    m_ssg3 = compute(I_ssg3)\n",
    "\n",
    "    row = {\n",
    "        \"name\": Path(d[\"path\"]).name,\n",
    "        \"success\": int(bool(d[\"success\"])),\n",
    "        \"drop\": float(d[\"drop\"]),\n",
    "        \"target\": float(tgt),\n",
    "        \"picked_conf\": float(info.get(\"picked_conf\", np.nan)),\n",
    "    }\n",
    "    for pref, mm in [(\"g\", m_g), (\"gxa\", m_gxa), (\"ssg3\", m_ssg3)]:\n",
    "        for k, v in mm.items():\n",
    "            row[f\"{pref}_{k}\"] = float(v)\n",
    "\n",
    "    # keep y for the chosen SSGrad top-3 target (sanity/debug)\n",
    "    row[\"ssg3_y\"] = float(y3)\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"Computed rows:\", len(df))\n",
    "print(df.head(3))\n",
    "\n",
    "# ---------- AUC summary + plots ----------\n",
    "families = {\"|grad|\": \"g\", \"|grad×act|\": \"gxa\", \"SSGrad top-3\": \"ssg3\"}\n",
    "metrics = [\"iou_topk\", \"cos\", \"pearson\", \"spearman\", \"delta_in_topI\"]\n",
    "\n",
    "auc_rows = []\n",
    "for fam, pref in families.items():\n",
    "    for m in metrics:\n",
    "        col = f\"{pref}_{m}\"\n",
    "        auc_rows.append({\"family\": fam, \"metric\": m, \"col\": col, \"auc\": auc_roc(df.success.values, df[col].values)})\n",
    "\n",
    "auc_df = pd.DataFrame(auc_rows).sort_values(\"auc\", ascending=False)\n",
    "print(\"\\nTop metrics by AUC:\")\n",
    "print(auc_df.head(12))\n",
    "\n",
    "# Bar plot of AUC\n",
    "plt.figure(figsize=(10, 4.8))\n",
    "order = np.argsort(-auc_df[\"auc\"].values)\n",
    "vals = auc_df[\"auc\"].values[order]\n",
    "labels = (auc_df[\"family\"].values[order] + \" : \" + auc_df[\"metric\"].values[order])\n",
    "plt.bar(np.arange(len(vals)), vals)\n",
    "plt.xticks(np.arange(len(vals)), labels, rotation=75, ha=\"right\")\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.title(\"ROC AUC by metric (higher is better)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# For each metric: boxplot + ROC"
   ],
   "id": "d95d7801be696e06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1f0d833619069114",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Канально-пространственные метрики: сравнение тензоров (C,20,20) вместо (1,20,20)\n",
    "\n",
    "Выше мы агрегировали карты по каналам (получали (1,20,20)), что действительно теряет информацию о том, **какие именно каналы** важны/атакуются.\n",
    "\n",
    "Ниже считаем метрики прямо по тензорам **(C,20,20)** для:\n",
    "- `Δ` — тензор изменённых фич: `Δ = A(patched) - A(clean)` на слое `model.22`.\n",
    "- `I` — тензор важности фич (канально-пространственный): используем `|grad×act|` как базовую importance-map в пространстве и по каналам: `I = |G ⊙ A_clean|`.\n",
    "\n",
    "Почему это полезно:\n",
    "- можно отделить случаи «атака попала в *те же каналы*» от случаев «попала в другие каналы, но суммарная энергия в ROI похожа»;\n",
    "- можно проверять **канальную согласованность** и **пространственную согласованность** отдельно;\n",
    "- можно смотреть **пер-канальные** совпадения (распределения cosine по каналам), а не только один скаляр.\n",
    "\n",
    "Метрики (все считаем на `abs`, если явно не указано иначе):\n",
    "1) `cos_flat_CHW` — cosine между `vec(|Δ|)` и `vec(I)` в размерности `C·H·W` (сохраняет каналы).\n",
    "2) `pearson_flat_CHW` / `spearman_flat_CHW` — корреляции по `C·H·W`.\n",
    "3) `chan_energy_spearman` / `chan_energy_pearson` — корреляции между векторами энергий каналов `eΔ[k]=mean|Δ_k|`, `eI[k]=mean I_k`.\n",
    "4) `chan_topk_jaccard` — пересечение топ-K каналов по энергии (какие каналы «атакуются» vs «важны»).\n",
    "5) `perchan_cos_mean/median/top10_mean/wmean` — распределение cosine по каналам (сводим статистиками; `wmean` взвешивает по `eI`).\n",
    "6) `sign_agreement` — доля совпадения знака между `Δ` и `(G⊙A)` (проверка на «negative evidence» / suppress-эффекты).\n",
    "\n",
    "Цель: найти метрики, которые лучше разделяют success vs fail и дают интерпретируемую структуру (каналы/пространство/знак)."
   ],
   "id": "d809737de8d46a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- channel-spatial helpers ----------\n",
    "\n",
    "def _chw_abs(t_bchw: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"(1,C,H,W) -> (C,H,W) abs float32 numpy.\"\"\"\n",
    "    return t_bchw.detach().cpu().abs().to(torch.float32)[0].numpy()\n",
    "\n",
    "\n",
    "def _chw_signed(t_bchw: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"(1,C,H,W) -> (C,H,W) signed float32 numpy.\"\"\"\n",
    "    return t_bchw.detach().cpu().to(torch.float32)[0].numpy()\n",
    "\n",
    "\n",
    "def _flat(x: np.ndarray) -> np.ndarray:\n",
    "    return x.reshape(-1).astype(np.float64)\n",
    "\n",
    "\n",
    "def _cos(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    a = _flat(a); b = _flat(b)\n",
    "    na = float(np.linalg.norm(a)); nb = float(np.linalg.norm(b))\n",
    "    if na < eps or nb < eps:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / (na * nb))\n",
    "\n",
    "\n",
    "def _pearson(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    a = _flat(a); b = _flat(b)\n",
    "    a = a - a.mean(); b = b - b.mean()\n",
    "    den = float(np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    if den < eps:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / den)\n",
    "\n",
    "\n",
    "def _spearman(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    a = _flat(a); b = _flat(b)\n",
    "    ra = a.argsort().argsort().astype(np.float64)\n",
    "    rb = b.argsort().argsort().astype(np.float64)\n",
    "    ra -= ra.mean(); rb -= rb.mean()\n",
    "    den = float(np.linalg.norm(ra) * np.linalg.norm(rb))\n",
    "    if den < eps:\n",
    "        return 0.0\n",
    "    return float(np.dot(ra, rb) / den)\n",
    "\n",
    "\n",
    "def _jaccard_topk_channels(ea: np.ndarray, eb: np.ndarray, k: int) -> float:\n",
    "    k = int(min(k, ea.size, eb.size))\n",
    "    ia = set(np.argsort(-ea)[:k].tolist())\n",
    "    ib = set(np.argsort(-eb)[:k].tolist())\n",
    "    inter = len(ia & ib)\n",
    "    uni = len(ia | ib)\n",
    "    return float(inter / max(1, uni))\n",
    "\n",
    "\n",
    "def per_channel_cosines(A: np.ndarray, B: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"A,B: (C,H,W) -> cos per channel (C,)\"\"\"\n",
    "    C = A.shape[0]\n",
    "    out = np.zeros((C,), dtype=np.float64)\n",
    "    for k in range(C):\n",
    "        a = A[k].reshape(-1).astype(np.float64)\n",
    "        b = B[k].reshape(-1).astype(np.float64)\n",
    "        na = float(np.linalg.norm(a)); nb = float(np.linalg.norm(b))\n",
    "        if na < eps or nb < eps:\n",
    "            out[k] = 0.0\n",
    "        else:\n",
    "            out[k] = float(np.dot(a, b) / (na * nb))\n",
    "    return out\n",
    "\n",
    "\n",
    "def sign_agreement(delta_signed: np.ndarray, imp_signed: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    \"\"\"Fraction of positions where signs agree, ignoring near-zeros.\"\"\"\n",
    "    a = delta_signed.reshape(-1)\n",
    "    b = imp_signed.reshape(-1)\n",
    "    mask = (np.abs(a) > eps) & (np.abs(b) > eps)\n",
    "    if mask.sum() == 0:\n",
    "        return float('nan')\n",
    "    return float((np.sign(a[mask]) == np.sign(b[mask])).mean())\n",
    "\n",
    "\n",
    "# ---------- build (C,20,20) metrics ----------\n",
    "TARGET_LAYER = \"model.22\"\n",
    "TOPK_CH = int(CFG.topk_channels) if hasattr(CFG, \"topk_channels\") else 32\n",
    "\n",
    "rows2 = []\n",
    "\n",
    "for d in run_data:\n",
    "    info = d.get(\"gradcam_info\", {})\n",
    "    bbox = info.get(\"picked_bbox\", None)\n",
    "    if bbox is None:\n",
    "        continue\n",
    "\n",
    "    # Need clean activation+grad on the same layer\n",
    "    x_clean = torch_preprocess_letterboxed(d[\"clean_lb\"], device=CFG.device, dtype=_MODEL_DTYPE)\n",
    "\n",
    "    # (1,C,20,20) activation/grad for importance target (we keep your bbox-conditioned target)\n",
    "    A, G, tgt = layer_grad_and_act_for_bbox_target(\n",
    "        model_torch,\n",
    "        x_clean,\n",
    "        target_layer_name=TARGET_LAYER,\n",
    "        target_class_id=int(target_class_id),\n",
    "        imgsz=int(CFG.imgsz),\n",
    "        bbox_xyxy=tuple(float(x) for x in bbox),\n",
    "    )\n",
    "\n",
    "    # Δ tensor from cached activations (patched-clean)\n",
    "    delta = d.get(\"deltas\", {}).get(TARGET_LAYER, None)\n",
    "    if delta is None:\n",
    "        continue\n",
    "    delta = delta.to(torch.float32)  # CPU (1,C,20,20)\n",
    "\n",
    "    # Importance tensor: |grad×act| (clean)\n",
    "    imp = (G * A).abs()  # (1,C,20,20)\n",
    "\n",
    "    # CHW numpy\n",
    "    D_abs = _chw_abs(delta)\n",
    "    I_abs = _chw_abs(imp)\n",
    "\n",
    "    D_signed = _chw_signed(delta)\n",
    "    I_signed = _chw_signed(G * A)  # signed evidence proxy\n",
    "\n",
    "    # Channel energies\n",
    "    eD = D_abs.mean(axis=(1, 2))  # (C,)\n",
    "    eI = I_abs.mean(axis=(1, 2))  # (C,)\n",
    "\n",
    "    # Per-channel spatial cosine distribution\n",
    "    cos_k = per_channel_cosines(D_abs, I_abs)\n",
    "    # focus on channels that matter for I (avoid dividing by ~0)\n",
    "    w = eI.astype(np.float64)\n",
    "    w = w / max(1e-12, float(w.sum()))\n",
    "\n",
    "    # Build metrics\n",
    "    m = {\n",
    "        \"cos_flat_CHW\": _cos(D_abs, I_abs),\n",
    "        \"pearson_flat_CHW\": _pearson(D_abs, I_abs),\n",
    "        \"spearman_flat_CHW\": _spearman(D_abs, I_abs),\n",
    "        \"chan_energy_pearson\": _pearson(eD, eI),\n",
    "        \"chan_energy_spearman\": _spearman(eD, eI),\n",
    "        \"chan_topk_jaccard\": _jaccard_topk_channels(eD, eI, k=TOPK_CH),\n",
    "        \"perchan_cos_mean\": float(np.mean(cos_k)),\n",
    "        \"perchan_cos_median\": float(np.median(cos_k)),\n",
    "        \"perchan_cos_top10_mean\": float(np.mean(np.sort(cos_k)[-10:])),\n",
    "        \"perchan_cos_wmean\": float(np.sum(w * cos_k)),\n",
    "        \"sign_agreement\": sign_agreement(D_signed, I_signed),\n",
    "    }\n",
    "\n",
    "    row = {\n",
    "        \"name\": Path(d[\"path\"]).name,\n",
    "        \"success\": int(bool(d[\"success\"])),\n",
    "        \"drop\": float(d[\"drop\"]),\n",
    "        \"target\": float(tgt),\n",
    "        \"picked_conf\": float(info.get(\"picked_conf\", np.nan)),\n",
    "        **{k: float(v) if (v is not None and not (isinstance(v, float) and np.isnan(v))) else float('nan') for k, v in m.items()},\n",
    "    }\n",
    "\n",
    "    rows2.append(row)\n",
    "\n",
    "import pandas as pd\n",
    "cdf = pd.DataFrame(rows2)\n",
    "print(\"Computed rows:\", len(cdf))\n",
    "print(cdf.head(3))\n",
    "\n",
    "# ---------- AUC summary + plots ----------\n",
    "cs_metrics = [\n",
    "    \"cos_flat_CHW\", \"pearson_flat_CHW\", \"spearman_flat_CHW\",\n",
    "    \"chan_energy_pearson\", \"chan_energy_spearman\", \"chan_topk_jaccard\",\n",
    "    \"perchan_cos_mean\", \"perchan_cos_median\", \"perchan_cos_top10_mean\", \"perchan_cos_wmean\",\n",
    "    \"sign_agreement\",\n",
    "]\n",
    "\n",
    "auc_rows2 = []\n",
    "for m in cs_metrics:\n",
    "    auc_rows2.append({\"metric\": m, \"auc\": auc_roc(cdf.success.values, cdf[m].values)})\n",
    "auc2 = pd.DataFrame(auc_rows2).sort_values(\"auc\", ascending=False)\n",
    "print(\"\\nTop (C,20,20) metrics by AUC:\")\n",
    "print(auc2.head(20))\n",
    "\n",
    "# bar plot\n",
    "plt.figure(figsize=(10, 4.8))\n",
    "order = np.argsort(-auc2[\"auc\"].values)\n",
    "vals = auc2[\"auc\"].values[order]\n",
    "labels = auc2[\"metric\"].values[order]\n",
    "plt.bar(np.arange(len(vals)), vals)\n",
    "plt.xticks(np.arange(len(vals)), labels, rotation=75, ha=\"right\")\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.title(\"ROC AUC for channel-spatial metrics (C,20,20)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# boxplot + ROC per metric\n",
    "for m in cs_metrics:\n",
    "    s0 = cdf[cdf.success == 0][m].dropna().values\n",
    "    s1 = cdf[cdf.success == 1][m].dropna().values\n",
    "    boxplot_metric(s0, s1, title=f\"(C,20,20): {m} (distribution)\", ylabel=m)\n",
    "    rocplot_metric(cdf.success.values.astype(np.int32), cdf[m].values.astype(np.float64), title=f\"(C,20,20): {m} (ROC)\")\n",
    "\n",
    "# Optional: show per-channel cosine histograms for a few examples\n",
    "SHOW_K = 3\n",
    "print(\"\\n[optional] Per-channel cosine histograms (first few examples):\")\n",
    "shown = 0\n",
    "for d in run_data:\n",
    "    if shown >= SHOW_K:\n",
    "        break\n",
    "    info = d.get(\"gradcam_info\", {})\n",
    "    bbox = info.get(\"picked_bbox\", None)\n",
    "    if bbox is None:\n",
    "        continue\n",
    "\n",
    "    x_clean = torch_preprocess_letterboxed(d[\"clean_lb\"], device=CFG.device, dtype=_MODEL_DTYPE)\n",
    "    A, G, tgt = layer_grad_and_act_for_bbox_target(\n",
    "        model_torch, x_clean, TARGET_LAYER, int(target_class_id), int(CFG.imgsz), tuple(float(x) for x in bbox)\n",
    "    )\n",
    "    delta = d.get(\"deltas\", {}).get(TARGET_LAYER, None)\n",
    "    if delta is None:\n",
    "        continue\n",
    "    delta = delta.to(torch.float32)\n",
    "\n",
    "    D_abs = _chw_abs(delta)\n",
    "    I_abs = _chw_abs((G * A).abs())\n",
    "\n",
    "    cos_k = per_channel_cosines(D_abs, I_abs)\n",
    "\n",
    "    plt.figure(figsize=(6.5, 3.8))\n",
    "    plt.hist(cos_k, bins=30)\n",
    "    plt.title(f\"Per-channel spatial cosine\\n{Path(d['path']).name} | success={bool(d['success'])} | drop={float(d['drop']):.3f}\")\n",
    "    plt.xlabel(\"cos(|Δ_k|, |G⊙A|_k)\"); plt.ylabel(\"count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    shown += 1"
   ],
   "id": "a0a63746a16c6c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def ssgradcam_raw(A_bchw: torch.Tensor, G_bchw: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    SSGrad-CAM per Yamauchi: L = ReLU( sum_k (w_k * A_k) ∘ S_k )\n",
    "    where:\n",
    "      w_k = GAP( dY/dA_k )\n",
    "      S_k = |dY/dA_k| / max(|dY/dA_k|)  (per-channel max over spatial dims)\n",
    "    Inputs:\n",
    "      A_bchw, G_bchw: (1,C,H,W)\n",
    "    Returns:\n",
    "      (1,1,H,W) normalized to [0,1]\n",
    "    \"\"\"\n",
    "    # channel weights (1,C,1,1)\n",
    "    w = G_bchw.mean(dim=(2, 3), keepdim=True)\n",
    "\n",
    "    # space maps (1,C,H,W) in [0,1]\n",
    "    S = G_bchw.abs()\n",
    "    S = S / (S.amax(dim=(2, 3), keepdim=True).clamp(min=eps))\n",
    "\n",
    "    # elementwise spatial gating per channel then sum channels\n",
    "    cam = ((w * A_bchw) * S).sum(dim=1, keepdim=True)\n",
    "    cam = F.relu(cam)\n",
    "    cam = torch.nan_to_num(cam, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # normalize to [0,1]\n",
    "    cam = cam - cam.amin(dim=(2, 3), keepdim=True)\n",
    "    cam = cam / cam.amax(dim=(2, 3), keepdim=True).clamp(min=eps)\n",
    "    return cam\n",
    "\n",
    "def _overlay_triplet(pil_img, h1, h2, h3, titles, imgsz=640):\n",
    "    img = np.asarray(pil_img.convert(\"RGB\"))\n",
    "\n",
    "    def up(h):\n",
    "        return F.interpolate(h.detach().cpu(), size=(imgsz, imgsz), mode=\"nearest\")[0,0].numpy()\n",
    "\n",
    "    hs = [up(h1), up(h2), up(h3)]\n",
    "\n",
    "    plt.figure(figsize=(18,5))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1,3,i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(hs[i], alpha=0.45, vmin=0.0, vmax=1.0)\n",
    "        plt.title(titles[i])\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---- RUN: compare Grad-CAM vs SSGrad-CAM vs |grad×act| on the SAME instance-specific y ----\n",
    "TARGET_LAYER = \"model.22\"\n",
    "N_EX = 5\n",
    "BETA_IOU = 0.0  # optionally try 0.5..1.0 (instance bbox term), but start with 0.0\n",
    "\n",
    "succ = [d for d in run_data if bool(d[\"success\"])]\n",
    "fail = [d for d in run_data if (not bool(d[\"success\"]))]\n",
    "examples = (succ[:N_EX] + fail[:N_EX])\n",
    "\n",
    "for d in examples:\n",
    "    info = d.get(\"gradcam_info\", {})\n",
    "    bbox = info.get(\"picked_bbox\", None)\n",
    "    if bbox is None:\n",
    "        continue\n",
    "\n",
    "    x_clean = torch_preprocess_letterboxed(d[\"clean_lb\"], device=CFG.device, dtype=_MODEL_DTYPE)\n",
    "\n",
    "    A, G, y = layer_grad_and_act_for_bbox_target(\n",
    "        model_torch,\n",
    "        x_clean,\n",
    "        target_layer_name=TARGET_LAYER,\n",
    "        target_class_id=int(target_class_id),\n",
    "        imgsz=int(CFG.imgsz),\n",
    "        bbox_xyxy=tuple(float(v) for v in bbox),\n",
    "    )\n",
    "\n",
    "    # maps on the same 20×20 grid\n",
    "    gc  = gradcam_raw_20(A, G)\n",
    "    ssg = ssgradcam_raw(A, G)\n",
    "    gxa = _normalize_2d((G * A).abs().sum(dim=1, keepdim=True))\n",
    "\n",
    "    title = (\n",
    "        f\"{Path(d['path']).name} | success={bool(d['success'])} | drop={float(d['drop']):.3f} | \"\n",
    "        f\"y={y:.4f}\"\n",
    "    )\n",
    "\n",
    "    print(title)\n",
    "    _overlay_triplet(\n",
    "        d[\"clean_lb\"],\n",
    "        gc, ssg, gxa,\n",
    "        titles=[\"Grad-CAM\", \"SSGrad-CAM\", \"|grad×act| (same y)\"],\n",
    "        imgsz=int(CFG.imgsz),\n",
    "    )"
   ],
   "id": "1812251eeeb953ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## SSGrad-CAM: сравнение разных определений y для детектора\n",
    "\n",
    "Ниже сравниваем **SSGrad-CAM** для четырёх определений целевого скаляра `y` (по которому берём градиент):\n",
    "\n",
    "1) **avg_roi_logits** — текущий вариант: усреднение *логитов person* внутри bbox (ROI-маска) по всем FPN-уровням.\n",
    "2) **global_max_prob** — самый уверенный человек на картинке: `max sigmoid(person_logit)` по всем уровням и клеткам.\n",
    "3) **topk10_roi_logits** — top-10 самых уверенных предсказаний person внутри bbox (ранжирование по `sigmoid`, но агрегируем *логиты*).\n",
    "4) **roi_prob_gt_thr_logits** — все предсказания person внутри bbox с `sigmoid(logit) > 0.3`, агрегируем *логиты*.\n",
    "\n",
    "Цель: проверить, что проблема Grad/SSGrad-CAM была (или не была) связана именно с выбором `y`."
   ],
   "id": "5848370d5bf80744"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ks = [1,3,10,30]\n",
    "for d in examples:\n",
    "    info = d.get(\"gradcam_info\", {})\n",
    "    bbox = info.get(\"picked_bbox\", None)\n",
    "    if bbox is None:\n",
    "        continue\n",
    "\n",
    "    x_clean = torch_preprocess_letterboxed(d[\"clean_lb\"], device=CFG.device, dtype=_MODEL_DTYPE)\n",
    "\n",
    "    maps = []\n",
    "    titles = []\n",
    "    y_dbg = []\n",
    "\n",
    "    for k in ks:\n",
    "        bb = tuple(float(v) for v in bbox)\n",
    "        A, G, y, yinfo = layer_grad_and_act_for_detector_target(\n",
    "            model_torch,\n",
    "            x_clean,\n",
    "            target_layer_name=TARGET_LAYER,\n",
    "            target_class_id=int(target_class_id),\n",
    "            imgsz=int(CFG.imgsz),\n",
    "            bbox_xyxy=bb,\n",
    "            topk=int(k),\n",
    "            thr=0.0,\n",
    "            mode=\"topk10_roi_logits\",\n",
    "        )\n",
    "        ssg = ssgradcam_raw(A, G)\n",
    "        maps.append(ssg)\n",
    "\n",
    "        titles.append(f\"top-{k} in ROI (logits)\\ny={y:.3f}\")\n",
    "\n",
    "    base_title = (\n",
    "        f\"{Path(d['path']).name} | success={bool(d['success'])} | drop={float(d['drop']):.3f} | \"\n",
    "        f\"picked_conf={float(info.get('picked_conf', np.nan)):.3f}\"\n",
    "    )\n",
    "\n",
    "    _overlay_four(d[\"clean_lb\"], maps, titles, suptitle=base_title, imgsz=int(CFG.imgsz))\n"
   ],
   "id": "da988e03ac15ba9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# ---------- channel-spatial helpers ----------\n",
    "\n",
    "def _chw_abs(t_bchw: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"(1,C,H,W) -> (C,H,W) abs float32 numpy.\"\"\"\n",
    "    return t_bchw.detach().cpu().abs().to(torch.float32)[0].numpy()\n",
    "\n",
    "\n",
    "def _chw_signed(t_bchw: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"(1,C,H,W) -> (C,H,W) signed float32 numpy.\"\"\"\n",
    "    return t_bchw.detach().cpu().to(torch.float32)[0].numpy()\n",
    "\n",
    "\n",
    "def _flat(x: np.ndarray) -> np.ndarray:\n",
    "    return x.reshape(-1).astype(np.float64)\n",
    "\n",
    "\n",
    "def _cos(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    a = _flat(a);\n",
    "    b = _flat(b)\n",
    "    na = float(np.linalg.norm(a));\n",
    "    nb = float(np.linalg.norm(b))\n",
    "    if na < eps or nb < eps:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / (na * nb))\n",
    "\n",
    "\n",
    "def _pearson(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    a = _flat(a);\n",
    "    b = _flat(b)\n",
    "    a = a - a.mean();\n",
    "    b = b - b.mean()\n",
    "    den = float(np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    if den < eps:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / den)\n",
    "\n",
    "\n",
    "def _spearman(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    a = _flat(a);\n",
    "    b = _flat(b)\n",
    "    ra = a.argsort().argsort().astype(np.float64)\n",
    "    rb = b.argsort().argsort().astype(np.float64)\n",
    "    ra -= ra.mean();\n",
    "    rb -= rb.mean()\n",
    "    den = float(np.linalg.norm(ra) * np.linalg.norm(rb))\n",
    "    if den < eps:\n",
    "        return 0.0\n",
    "    return float(np.dot(ra, rb) / den)\n",
    "\n",
    "\n",
    "def _jaccard_topk_channels(ea: np.ndarray, eb: np.ndarray, k: int) -> float:\n",
    "    k = int(min(k, ea.size, eb.size))\n",
    "    ia = set(np.argsort(-ea)[:k].tolist())\n",
    "    ib = set(np.argsort(-eb)[:k].tolist())\n",
    "    inter = len(ia & ib)\n",
    "    uni = len(ia | ib)\n",
    "    return float(inter / max(1, uni))\n",
    "\n",
    "\n",
    "def per_channel_cosines(A: np.ndarray, B: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"A,B: (C,H,W) -> cos per channel (C,)\"\"\"\n",
    "    C = A.shape[0]\n",
    "    out = np.zeros((C,), dtype=np.float64)\n",
    "    for k in range(C):\n",
    "        a = A[k].reshape(-1).astype(np.float64)\n",
    "        b = B[k].reshape(-1).astype(np.float64)\n",
    "        na = float(np.linalg.norm(a));\n",
    "        nb = float(np.linalg.norm(b))\n",
    "        if na < eps or nb < eps:\n",
    "            out[k] = 0.0\n",
    "        else:\n",
    "            out[k] = float(np.dot(a, b) / (na * nb))\n",
    "    return out\n",
    "\n",
    "\n",
    "def sign_agreement(delta_signed: np.ndarray, imp_signed: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    \"\"\"Fraction of positions where signs agree, ignoring near-zeros.\"\"\"\n",
    "    a = delta_signed.reshape(-1)\n",
    "    b = imp_signed.reshape(-1)\n",
    "    mask = (np.abs(a) > eps) & (np.abs(b) > eps)\n",
    "    if mask.sum() == 0:\n",
    "        return float('nan')\n",
    "    return float((np.sign(a[mask]) == np.sign(b[mask])).mean())\n",
    "\n",
    "\n",
    "# ---------- build (C,20,20) metrics ----------\n",
    "TARGET_LAYER = \"model.22\"\n",
    "TOPK_CH = int(CFG.topk_channels) if hasattr(CFG, \"topk_channels\") else 32\n",
    "\n",
    "rows2 = []\n",
    "\n",
    "for d in run_data:\n",
    "    info = d.get(\"gradcam_info\", {})\n",
    "    bbox = info.get(\"picked_bbox\", None)\n",
    "    if bbox is None:\n",
    "        continue\n",
    "\n",
    "    # Need clean activation+grad on the same layer\n",
    "    x_clean = torch_preprocess_letterboxed(d[\"clean_lb\"], device=CFG.device, dtype=_MODEL_DTYPE)\n",
    "\n",
    "    # (1,C,20,20) activation/grad for importance target (we keep your bbox-conditioned target)\n",
    "    A, G, tgt = layer_grad_and_act_for_bbox_target(\n",
    "        model_torch,\n",
    "        x_clean,\n",
    "        target_layer_name=TARGET_LAYER,\n",
    "        target_class_id=int(target_class_id),\n",
    "        imgsz=int(CFG.imgsz),\n",
    "        bbox_xyxy=tuple(float(x) for x in bbox),\n",
    "    )\n",
    "\n",
    "    # Δ tensor from cached activations (patched-clean)\n",
    "    delta = d.get(\"deltas\", {}).get(TARGET_LAYER, None)\n",
    "    if delta is None:\n",
    "        continue\n",
    "    delta = delta.to(torch.float32)  # CPU (1,C,20,20)\n",
    "\n",
    "    # Importance tensor: |grad×act| (clean)\n",
    "    imp = (G * A).abs()  # (1,C,20,20)\n",
    "\n",
    "    # CHW numpy\n",
    "    D_abs = _chw_abs(delta)\n",
    "    I_abs = _chw_abs(imp)\n",
    "\n",
    "    D_signed = _chw_signed(delta)\n",
    "    I_signed = _chw_signed(G * A)  # signed evidence proxy\n",
    "\n",
    "    # Channel energies\n",
    "    eD = D_abs.mean(axis=(1, 2))  # (C,)\n",
    "    eI = I_abs.mean(axis=(1, 2))  # (C,)\n",
    "\n",
    "    # Per-channel spatial cosine distribution\n",
    "    cos_k = per_channel_cosines(D_abs, I_abs)\n",
    "    # focus on channels that matter for I (avoid dividing by ~0)\n",
    "    w = eI.astype(np.float64)\n",
    "    w = w / max(1e-12, float(w.sum()))\n",
    "\n",
    "    # Build metrics\n",
    "    m = {\n",
    "        \"cos_flat_CHW\": _cos(D_abs, I_abs),\n",
    "        \"pearson_flat_CHW\": _pearson(D_abs, I_abs),\n",
    "        \"spearman_flat_CHW\": _spearman(D_abs, I_abs),\n",
    "        \"chan_energy_pearson\": _pearson(eD, eI),\n",
    "        \"chan_energy_spearman\": _spearman(eD, eI),\n",
    "        \"chan_topk_jaccard\": _jaccard_topk_channels(eD, eI, k=TOPK_CH),\n",
    "        \"perchan_cos_mean\": float(np.mean(cos_k)),\n",
    "        \"perchan_cos_median\": float(np.median(cos_k)),\n",
    "        \"perchan_cos_top10_mean\": float(np.mean(np.sort(cos_k)[-10:])),\n",
    "        \"perchan_cos_wmean\": float(np.sum(w * cos_k)),\n",
    "        \"sign_agreement\": sign_agreement(D_signed, I_signed),\n",
    "    }\n",
    "\n",
    "    row = {\n",
    "        \"name\": Path(d[\"path\"]).name,\n",
    "        \"success\": int(bool(d[\"success\"])),\n",
    "        \"drop\": float(d[\"drop\"]),\n",
    "        \"target\": float(tgt),\n",
    "        \"picked_conf\": float(info.get(\"picked_conf\", np.nan)),\n",
    "        **{k: float(v) if (v is not None and not (isinstance(v, float) and np.isnan(v))) else float('nan') for k, v in\n",
    "           m.items()},\n",
    "    }\n",
    "\n",
    "    rows2.append(row)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "cdf = pd.DataFrame(rows2)\n",
    "print(\"Computed rows:\", len(cdf))\n",
    "print(cdf.head(3))\n",
    "\n",
    "# ---------- AUC summary + plots ----------\n",
    "cs_metrics = [\n",
    "    \"cos_flat_CHW\", \"pearson_flat_CHW\", \"spearman_flat_CHW\",\n",
    "    \"chan_energy_pearson\", \"chan_energy_spearman\", \"chan_topk_jaccard\",\n",
    "    \"perchan_cos_mean\", \"perchan_cos_median\", \"perchan_cos_top10_mean\", \"perchan_cos_wmean\",\n",
    "    \"sign_agreement\",\n",
    "]\n",
    "\n",
    "auc_rows2 = []\n",
    "for m in cs_metrics:\n",
    "    auc_rows2.append({\"metric\": m, \"auc\": auc_roc(cdf.success.values, cdf[m].values)})\n",
    "auc2 = pd.DataFrame(auc_rows2).sort_values(\"auc\", ascending=False)\n",
    "print(\"\\nTop (C,20,20) metrics by AUC:\")\n",
    "print(auc2.head(20))\n",
    "\n",
    "# bar plot\n",
    "plt.figure(figsize=(10, 4.8))\n",
    "order = np.argsort(-auc2[\"auc\"].values)\n",
    "vals = auc2[\"auc\"].values[order]\n",
    "labels = auc2[\"metric\"].values[order]\n",
    "plt.bar(np.arange(len(vals)), vals)\n",
    "plt.xticks(np.arange(len(vals)), labels, rotation=75, ha=\"right\")\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.title(\"ROC AUC for channel-spatial metrics (C,20,20)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# boxplot + ROC per metric\n",
    "for m in cs_metrics:\n",
    "    s0 = cdf[cdf.success == 0][m].dropna().values\n",
    "    s1 = cdf[cdf.success == 1][m].dropna().values\n",
    "    boxplot_metric(s0, s1, title=f\"(C,20,20): {m} (distribution)\", ylabel=m)\n",
    "    rocplot_metric(cdf.success.values.astype(np.int32), cdf[m].values.astype(np.float64), title=f\"(C,20,20): {m} (ROC)\")\n",
    "\n",
    "# Optional: show per-channel cosine histograms for a few examples\n",
    "SHOW_K = 3\n",
    "print(\"\\n[optional] Per-channel cosine histograms (first few examples):\")\n",
    "shown = 0\n",
    "for d in run_data:\n",
    "    if shown >= SHOW_K:\n",
    "        break\n",
    "    info = d.get(\"gradcam_info\", {})\n",
    "    bbox = info.get(\"picked_bbox\", None)\n",
    "    if bbox is None:\n",
    "        continue\n",
    "\n",
    "    x_clean = torch_preprocess_letterboxed(d[\"clean_lb\"], device=CFG.device, dtype=_MODEL_DTYPE)\n",
    "    A, G, tgt = layer_grad_and_act_for_bbox_target(\n",
    "        model_torch, x_clean, TARGET_LAYER, int(target_class_id), int(CFG.imgsz), tuple(float(x) for x in bbox)\n",
    "    )\n",
    "    delta = d.get(\"deltas\", {}).get(TARGET_LAYER, None)\n",
    "    if delta is None:\n",
    "        continue\n",
    "    delta = delta.to(torch.float32)\n",
    "\n",
    "    D_abs = _chw_abs(delta)\n",
    "    I_abs = _chw_abs((G * A).abs())\n",
    "\n",
    "    cos_k = per_channel_cosines(D_abs, I_abs)\n",
    "\n",
    "    plt.figure(figsize=(6.5, 3.8))\n",
    "    plt.hist(cos_k, bins=30)\n",
    "    plt.title(\n",
    "        f\"Per-channel spatial cosine\\n{Path(d['path']).name} | success={bool(d['success'])} | drop={float(d['drop']):.3f}\")\n",
    "    plt.xlabel(\"cos(|Δ_k|, |G⊙A|_k)\");\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    shown += 1\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ],
   "id": "30aca0ac82b80d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "71971c2d5fff7c6f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
